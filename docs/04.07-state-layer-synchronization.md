# 7. State Synchronization

## 7.1 Fast Sync Mechanisms
Optimized ways to synchronize state between nodes.

```rust
struct StateSynchronizer {
    network_service: Arc<NetworkService>,
    storage_service: Arc<StorageService>,
    state_manager: Arc<StateManager>,
    config: StateSyncConfig,
}

struct StateSyncConfig {
    max_concurrent_requests: usize,
    target_peer_count: usize,
    request_timeout: Duration,
    max_batch_size: usize,
    max_retries: u32,
}

struct SyncProgress {
    target_state_root: Hash256,
    target_height: BlockHeight,
    nodes_processed: usize,
    nodes_remaining: usize,
    bytes_downloaded: usize,
    started_at: Instant,
}

impl StateSynchronizer {
    async fn sync_to_state(&self, target_root: Hash256, target_height: BlockHeight) -> Result<()> {
        log::info!("Starting state sync to root {} at height {}", 
                 target_root, target_height);
        
        // Find peers that have the target state
        let peers = self.find_peers_with_state(target_root).await?;
        
        if peers.is_empty() {
            return Err(Error::NoPeersWithTargetState);
        }
        
        // Queue of state trie nodes to fetch
        let mut queue = VecDeque::new();
        queue.push_back(target_root);
        
        // Track nodes we've already processed
        let mut processed = HashSet::new();
        let mut progress = SyncProgress {
            target_state_root: target_root,
            target_height,
            nodes_processed: 0,
            nodes_remaining: 1, // Start with just the root
            bytes_downloaded: 0,
            started_at: Instant::now(),
        };
        
        // Process queue until empty
        while !queue.is_empty() {
            // Take a batch of nodes to fetch
            let batch_size = min(queue.len(), self.config.max_batch_size);
            let mut batch = Vec::with_capacity(batch_size);
            
            for _ in 0..batch_size {
                if let Some(hash) = queue.pop_front() {
                    if !processed.contains(&hash) {
                        batch.push(hash);
                    }
                } else {
                    break;
                }
            }
            
            if batch.is_empty() {
                continue;
            }
            
            // Fetch nodes from network
            let nodes_data = self.fetch_state_nodes(&batch, &peers).await?;
            
            // Process fetched nodes
            for (hash, node_data) in nodes_data {
                // Store node
                self.storage_service.store_state_node(&hash, &node_data)?;
                
                // Parse node to find children
                let node = TrieNode::decode(&node_data)?;
                let child_hashes = node.get_child_references();
                
                // Add children to queue
                for child_hash in child_hashes {
                    if !processed.contains(&child_hash) {
                        queue.push_back(child_hash);
                    }
                }
                
                // Mark as processed
                processed.insert(hash);
                
                // Update progress
                progress.nodes_processed += 1;
                progress.nodes_remaining = processed.len() + queue.len() - progress.nodes_processed;
                progress.bytes_downloaded += node_data.len();
            }
            
            // Log progress periodically
            if progress.nodes_processed % 1000 == 0 {
                self.log_sync_progress(&progress);
            }
        }
        
        // Set current state root
        self.state_manager.set_state_root(target_root, target_height)?;
        
        log::info!("State sync completed: processed {} nodes ({} MB) in {:?}", 
                 progress.nodes_processed, 
                 progress.bytes_downloaded / (1024 * 1024),
                 progress.started_at.elapsed());
        
        Ok(())
    }
    
    async fn fetch_state_nodes(
        &self, 
        hashes: &[Hash256], 
        peers: &[PeerId]
    ) -> Result<HashMap<Hash256, Vec<u8>>> {
        // Simple round-robin peer selection
        let mut results = HashMap::new();
        let mut retry_count = 0;
        
        let mut remaining_hashes = hashes.to_vec();
        
        while !remaining_hashes.is_empty() && retry_count < self.config.max_retries {
            let peer_index = retry_count % peers.len();
            let peer = peers[peer_index];
            
            // Request batch of nodes
            let request = NetworkRequest::GetStateNodes { hashes: remaining_hashes.clone() };
            
            match timeout(
                self.config.request_timeout,
                self.network_service.send_request(peer, request)
            ).await {
                Ok(Ok(NetworkResponse::StateNodes { nodes })) => {
                    // Process successful responses
                    for (hash, data) in nodes {
                        results.insert(hash, data);
                    }
                    
                    // Remove successful hashes from remaining
                    remaining_hashes.retain(|h| !results.contains_key(h));
                }
                _ => {
                    // Request failed, try another peer
                    retry_count += 1;
                    log::debug!("State node request to peer {} failed, retrying ({}/{})",
                              peer, retry_count, self.config.max_retries);
                }
            }
        }
        
        if !remaining_hashes.is_empty() {
            return Err(Error::FailedToFetchStateNodes);
        }
        
        Ok(results)
    }
    
    fn log_sync_progress(&self, progress: &SyncProgress) {
        let elapsed = progress.started_at.elapsed().as_secs_f64();
        let nodes_per_sec = progress.nodes_processed as f64 / elapsed;
        let percent_complete = if progress.nodes_processed + progress.nodes_remaining > 0 {
            progress.nodes_processed as f64 * 100.0 / 
            (progress.nodes_processed + progress.nodes_remaining) as f64
        } else {
            0.0
        };
        
        log::info!("State sync progress: {:.2}% ({} nodes, {:.2} nodes/sec, {:.2} MB)",
                 percent_complete,
                 progress.nodes_processed,
                 nodes_per_sec,
                 progress.bytes_downloaded as f64 / (1024.0 * 1024.0));
    }
}
```

**Design Rationale**:
- **Breadth-First Traversal**: Efficiently explores the state trie
- **Parallel Network Requests**: Multiple concurrent requests for state nodes
- **Robust Retry Logic**: Handles network failures and peer disconnections
- **Progress Tracking**: Detailed metrics for sync monitoring

**For Beginners**: Fast sync is like downloading a snapshot of the current blockchain state instead of rebuilding it from scratch. It's similar to downloading a ZIP file of a website rather than visiting every page one by one - much faster for new nodes joining the network.

## 7.2 State Recovery
Mechanisms for recovering from incomplete or corrupted state.

```rust
struct StateRecoveryManager {
    storage_service: Arc<StorageService>,
    state_manager: Arc<StateManager>,
    network_service: Arc<NetworkService>,
    snapshot_manager: Arc<SnapshotManager>,
    config: RecoveryConfig,
}

struct RecoveryConfig {
    max_block_replay: u64,
    recovery_strategies: Vec<RecoveryStrategy>,
    integrity_check_depth: u32,
}

enum RecoveryStrategy {
    LocalSnapshot,
    PeerStateSync,
    BlockReplay,
    NetworkSnapshot,
}

impl StateRecoveryManager {
    async fn recover_state(&self, target_height: BlockHeight) -> Result<Hash256> {
        log::info!("Starting state recovery to height {}", target_height);
        
        // Try each recovery strategy in order
        for strategy in &self.config.recovery_strategies {
            match strategy {
                RecoveryStrategy::LocalSnapshot => {
                    if let Ok(root) = self.recover_from_local_snapshot(target_height).await {
                        log::info!("Successfully recovered state from local snapshot");
                        return Ok(root);
                    }
                },
                RecoveryStrategy::PeerStateSync => {
                    if let Ok(root) = self.recover_from_peer_state(target_height).await {
                        log::info!("Successfully recovered state from peer sync");
                        return Ok(root);
                    }
                },
                RecoveryStrategy::BlockReplay => {
                    if let Ok(root) = self.recover_by_replaying_blocks(target_height).await {
                        log::info!("Successfully recovered state by replaying blocks");
                        return Ok(root);
                    }
                },
                RecoveryStrategy::NetworkSnapshot => {
                    if let Ok(root) = self.recover_from_network_snapshot(target_height).await {
                        log::info!("Successfully recovered state from network snapshot");
                        return Ok(root);
                    }
                },
            }
        }
        
        Err(Error::StateRecoveryFailed)
    }
    
    async fn recover_from_local_snapshot(&self, target_height: BlockHeight) -> Result<Hash256> {
        // Find best snapshot at or before target height
        let snapshot = self.snapshot_manager.find_best_snapshot_before(target_height)?;
        
        if snapshot.height == target_height {
            // Exact match, just use the snapshot
            return Ok(snapshot.state_root);
        } else if snapshot.height < target_height {
            // Load snapshot state
            self.snapshot_manager.restore_snapshot(&snapshot.id)?;
            
            // Then replay blocks from snapshot to target
            return self.replay_blocks_range(snapshot.height, target_height).await;
        }
        
        Err(Error::NoSuitableSnapshot)
    }
    
    async fn recover_from_peer_state(&self, target_height: BlockHeight) -> Result<Hash256> {
        // Get block at target height to find state root
        let block = self.storage_service.get_block_by_height(target_height)?;
        
        // Use state synchronizer to fetch the state
        let state_sync = StateSynchronizer::new(
            self.network_service.clone(),
            self.storage_service.clone(),
            self.state_manager.clone(),
            StateSyncConfig::default(),
        );
        
        state_sync.sync_to_state(block.header.state_root, target_height).await?;
        
        // Verify integrity of synced state
        self.verify_state_integrity(block.header.state_root)?;
        
        Ok(block.header.state_root)
    }
    
    async fn replay_blocks_range(
        &self, 
        start_height: BlockHeight, 
        end_height: BlockHeight
    ) -> Result<Hash256> {
        // Ensure we have a valid start state
        let start_block = self.storage_service.get_block_by_height(start_height)?;
        let mut state_root = start_block.header.state_root;
        
        let mut state_processor = StateProcessor::new(
            self.storage_service.clone(),
            self.state_manager.clone(),
        );
        
        // Initialize state processor with starting state
        state_processor.initialize_with_state_root(state_root)?;
        
        // Replay each block sequentially
        for height in (start_height + 1)..=end_height {
            let block = self.storage_service.get_block_by_height(height)?;
            
            log::debug!("Replaying block {} during state recovery", height);
            state_root = state_processor.process_block(&block)?;
            
            // Verify the resulting state root matches the one in the block
            if state_root != block.header.state_root {
                log::error!("State root mismatch during replay at height {}", height);
                return Err(Error::StateRootMismatch {
                    expected: block.header.state_root,
                    actual: state_root,
                });
            }
        }
        
        Ok(state_root)
    }
    
    fn verify_state_integrity(&self, state_root: Hash256) -> Result<()> {
        log::debug!("Verifying integrity of state with root {}", state_root);
        
        // Create a state accessor with the target root
        let mut state_accessor = StateAccessor::new(
            self.storage_service.clone(),
            state_root
        );
        
        // Sample some accounts to check integrity
        // In a real implementation, we'd select a few key accounts
        // and verify their data is accessible and correct
        let accounts = self.select_accounts_for_verification();
        
        for address in accounts {
            if let Err(e) = state_accessor.get_account(&address) {
                log::error!("Failed to access account {} in state: {}", address, e);
                return Err(Error::StateIntegrityCheckFailed);
            }
        }
        
        // Verify root node is accessible and valid
        if let Err(e) = state_accessor.get_trie_node(&state_root) {
            log::error!("Failed to access state root node: {}", e);
            return Err(Error::StateIntegrityCheckFailed);
        }
        
        Ok(())
    }
}
```

**Design Rationale**:
- **Multiple Recovery Strategies**: Prioritized approaches for different scenarios
- **Graceful Degradation**: Falls back to slower methods if faster ones fail
- **Data Integrity Verification**: Ensures recovered state is valid
- **Minimal Downtime**: Focuses on quick restoration of service

**For Beginners**: State recovery is like having backup systems when your computer crashes - it ensures the blockchain can restore its data even after serious problems. Like having multiple ways to restore your computer (backups, recovery disks, cloud recovery), the blockchain has several strategies to rebuild its state when needed.

## 7.3 Consistency Checks
Mechanisms to ensure state remains consistent across the network.

```rust
struct ConsistencyChecker {
    state_manager: Arc<StateManager>,
    storage_service: Arc<StorageService>,
    network_service: Arc<NetworkService>,
    config: ConsistencyConfig,
}

struct ConsistencyConfig {
    check_interval: Duration,
    state_root_sample_size: usize,
    peer_sample_size: usize,
    account_sample_size: usize,
    auto_repair: bool,
}

struct ConsistencyCheckResult {
    checked_at: SystemTime,
    height_checked: BlockHeight,
    state_root_matches: bool,
    account_matches: bool,
    discrepancies: Vec<Discrepancy>,
    checked_against_peers: Vec<PeerId>,
}

enum Discrepancy {
    StateRootMismatch {
        local_root: Hash256,
        peer_roots: HashMap<PeerId, Hash256>,
    },
    AccountMismatch {
        address: Address,
        local_data: Option<Account>,
        peer_data: HashMap<PeerId, Option<Account>>,
    },
    StorageMismatch {
        address: Address,
        key: StorageKey,
        local_value: Option<StorageValue>,
        peer_values: HashMap<PeerId, Option<StorageValue>>,
    },
}

impl ConsistencyChecker {
    async fn run_periodic_check(&self) -> Result<ConsistencyCheckResult> {
        // Determine current height for checking
        let current_height = self.state_manager.get_current_height()?;
        
        // Get local state root at this height
        let local_state_root = self.state_manager.get_state_root_at_height(current_height)?;
        
        // Select peers to check against
        let peers = self.select_peers_for_consistency_check().await?;
        
        if peers.is_empty() {
            return Err(Error::NoPeersForConsistencyCheck);
        }
        
        // Initialize result
        let mut result = ConsistencyCheckResult {
            checked_at: SystemTime::now(),
            height_checked: current_height,
            state_root_matches: true,
            account_matches: true,
            discrepancies: Vec::new(),
            checked_against_peers: peers.clone(),
        };
        
        // Check state roots
        let peer_state_roots = self.check_state_roots_against_peers(
            current_height, &local_state_root, &peers
        ).await?;
        
        // If any mismatches, record discrepancy
        if peer_state_roots.values().any(|root| root != &local_state_root) {
            result.state_root_matches = false;
            result.discrepancies.push(Discrepancy::StateRootMismatch {
                local_root: local_state_root,
                peer_roots: peer_state_roots.clone(),
            });
        }
        
        // If state roots match, check sample accounts
        if result.state_root_matches {
            // Select accounts to verify
            let accounts = self.select_accounts_for_verification(current_height)?;
            
            // Check each account
            for address in accounts {
                let account_check = self.check_account_against_peers(
                    &address, &local_state_root, &peers
                ).await?;
                
                if let Some(discrepancy) = account_check {
                    result.account_matches = false;
                    result.discrepancies.push(discrepancy);
                }
            }
        }
        
        // If discrepancies found and auto-repair is enabled
        if !result.discrepancies.is_empty() && self.config.auto_repair {
            self.attempt_repair(&result).await?;
        }
        
        Ok(result)
    }
    
    async fn check_state_roots_against_peers(
        &self,
        height: BlockHeight,
        local_root: &Hash256,
        peers: &[PeerId]
    ) -> Result<HashMap<PeerId, Hash256>> {
        let mut peer_state_roots = HashMap::new();
        
        for &peer in peers {
            // Request state root at height from this peer
            let request = NetworkRequest::GetStateRootAtHeight { height };
            
            match self.network_service.send_request(peer, request).await {
                Ok(NetworkResponse::StateRoot { root }) => {
                    peer_state_roots.insert(peer, root);
                },
                _ => {
                    log::debug!("Failed to get state root from peer {}", peer);
                }
            }
        }
        
        Ok(peer_state_roots)
    }
    
    async fn check_account_against_peers(
        &self,
        address: &Address,
        state_root: &Hash256,
        peers: &[PeerId]
    ) -> Result<Option<Discrepancy>> {
        // Get local account data
        let local_account = self.state_manager.get_account_at_root(address, state_root)?;
        
        let mut peer_accounts = HashMap::new();
        let mut mismatch_detected = false;
        
        // Request same account from peers
        for &peer in peers {
            let request = NetworkRequest::GetAccountAtStateRoot { 
                address: *address, 
                state_root: *state_root 
            };
            
            match self.network_service.send_request(peer, request).await {
                Ok(NetworkResponse::AccountData { account }) => {
                    peer_accounts.insert(peer, account);
                    
                    // Check if this peer's account data matches our local account
                    if account != local_account {
                        mismatch_detected = true;
                    }
                },
                _ => {
                    log::debug!("Failed to get account data from peer {}", peer);
                }
            }
        }
        
        // If any mismatches found, return discrepancy
        if mismatch_detected {
            return Ok(Some(Discrepancy::AccountMismatch {
                address: *address,
                local_data: local_account,
                peer_data: peer_accounts,
            }));
        }
        
        Ok(None)
    }
    
    async fn attempt_repair(&self, result: &ConsistencyCheckResult) -> Result<()> {
        log::warn!("Attempting to repair state inconsistencies");
        
        for discrepancy in &result.discrepancies {
            match discrepancy {
                Discrepancy::StateRootMismatch { local_root, peer_roots } => {
                    // Determine the majority state root
                    let majority_root = self.find_majority_state_root(&peer_roots)?;
                    
                    if &majority_root != local_root {
                        log::warn!("Local state root differs from network majority. Initiating state sync.");
                        
                        // Use state synchronizer to repair
                        let state_sync = StateSynchronizer::new(
                            self.network_service.clone(),
                            self.storage_service.clone(),
                            self.state_manager.clone(),
                            StateSyncConfig::default(),
                        );
                        
                        state_sync.sync_to_state(majority_root, result.height_checked).await?;
                    }
                },
                // Handle other types of discrepancies
                _ => {
                    // For other discrepancies, we might repair individual accounts
                    // Implementation omitted for brevity
                }
            }
        }
        
        Ok(())
    }
}
```

**Design Rationale**:
- **Sampling Approach**: Partial checks balance thoroughness with efficiency
- **Peer Consensus**: Uses majority voting to determine correct state
- **Targeted Repair**: Fixes specific discrepancies without full resync
- **Proactive Monitoring**: Catches issues before they affect operations

**For Beginners**: Consistency checks are like periodically comparing notes with your classmates to make sure everyone has the same information. The blockchain occasionally verifies that its view of accounts and balances matches what other nodes have, and if there are differences, it can fix them before they cause problems.

[Back to Index](./04-0-state-layer-index.md) | [Previous: Smart Contract State](./04.06-state-layer-contracts.md) | [Next: State Verification](./04.08-state-layer-verification.md)
