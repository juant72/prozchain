# Rate Limiting & Caching

## Overview

ProzChain's API implements rate limiting and caching mechanisms to ensure optimal performance, stability, and fair resource allocation. This document explains how these mechanisms work and how to optimize your application to work effectively within these constraints.

## Rate Limiting

### Purpose

Rate limiting serves several important purposes:

- Prevents any single client from overwhelming the API
- Ensures fair resource distribution among all users
- Protects against denial-of-service (DoS) attacks
- Helps maintain API stability and performance

### Rate Limit Structure

ProzChain uses a tiered rate limiting system with varying limits based on:

1. **API Key Tier** - Different subscription levels have different limits
2. **Endpoint Sensitivity** - Resource-intensive endpoints have stricter limits
3. **Method Type** - Read vs. write operations have different limits

#### Tier-Based Limits

| Tier | Requests per Minute | Requests per Day | Concurrent Connections |
|------|---------------------|------------------|-------------------------|
| Free | 60                  | 10,000           | 5                       |
| Standard | 300              | 100,000         | 25                      |
| Professional | 1,200        | 500,000         | 100                     |
| Enterprise | Custom         | Custom           | Custom                  |

#### Endpoint-Specific Limits

Some endpoints have specific rate limits regardless of tier:

| Endpoint | Limit | Reason |
|----------|-------|--------|
| `/v1/transactions/submit` | 10/min | Resource-intensive write operation |
| `/v1/gas/price` | 120/min | Frequently accessed but light endpoint |
| GraphQL complex queries | 30/min | Potential for heavy computation |

### Rate Limit Headers

The API provides rate limit information through HTTP headers:

```http
X-RateLimit-Limit: 300
X-RateLimit-Remaining: 297
X-RateLimit-Reset: 1635794236
```

These headers inform you about:
- `X-RateLimit-Limit`: Total requests allowed in the current time window
- `X-RateLimit-Remaining`: Remaining requests in the current window
- `X-RateLimit-Reset`: Unix timestamp when the limit resets

### Exceeded Rate Limit Response

When you exceed a rate limit, the API returns a `429 Too Many Requests` response:

```json
{
  "error": {
    "code": "RATE_LIMIT_EXCEEDED",
    "message": "Rate limit exceeded. Please slow down your requests.",
    "details": {
      "retryAfter": 45,
      "limit": 300,
      "period": "1m"
    }
  }
}
```

The `retryAfter` value indicates the number of seconds to wait before making another request.

### Best Practices for Rate Limit Management

#### Client-Side Rate Limiting

Implement a rate limiter in your application:

```javascript
class RateLimiter {
  constructor(limit, interval) {
    this.limit = limit;
    this.interval = interval;
    this.tokens = limit;
    this.lastRefill = Date.now();
    this.queue = [];
  }
  
  async acquire() {
    // Refill tokens based on elapsed time
    const now = Date.now();
    const elapsed = now - this.lastRefill;
    if (elapsed > 0) {
      const refill = Math.floor(elapsed / this.interval) * this.limit;
      this.tokens = Math.min(this.limit, this.tokens + refill);
      this.lastRefill = now - (elapsed % this.interval);
    }
    
    if (this.tokens > 0) {
      this.tokens--;
      return Promise.resolve();
    } else {
      // No tokens available, queue the request
      return new Promise(resolve => {
        this.queue.push(resolve);
        
        // Set timeout to resolve after token is available
        const delay = this.interval - (now - this.lastRefill);
        setTimeout(() => {
          if (this.queue.length > 0) {
            this.tokens = Math.min(this.limit - 1, this.tokens);
            const next = this.queue.shift();
            next();
          }
        }, delay);
      });
    }
  }
}

// Usage example
const rateLimiter = new RateLimiter(300, 60000); // 300 requests per minute

async function makeApiCall() {
  await rateLimiter.acquire(); // Wait for token
  
  try {
    return await fetch('https://api.prozchain.com/v1/blocks/latest');
  } catch (error) {
    // Handle error
    throw error;
  }
}
```

#### Exponential Backoff

Implement exponential backoff for retrying failed requests:

```javascript
async function fetchWithRetry(url, options = {}, retries = 3) {
  const { maxRetries = 3, initialDelay = 1000, maxDelay = 30000 } = options;
  
  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      const response = await fetch(url, options);
      
      // Handle rate limiting
      if (response.status === 429) {
        const retryAfter = parseInt(response.headers.get('Retry-After') || '1', 10);
        const delay = Math.min(retryAfter * 1000, maxDelay);
        
        console.log(`Rate limited. Retrying after ${delay}ms`);
        await new Promise(resolve => setTimeout(resolve, delay));
        continue;
      }
      
      return response;
    } catch (error) {
      if (attempt === maxRetries) throw error;
      
      // Calculate exponential backoff delay
      const delay = Math.min(
        initialDelay * Math.pow(2, attempt),
        maxDelay
      );
      
      console.log(`Request failed. Retrying in ${delay}ms...`);
      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }
}
```

#### Request Batching

Batch multiple requests into a single call when possible:

```javascript
// Instead of multiple individual requests
async function getMultipleBalancesIndividually(addresses) {
  const balances = {};
  
  for (const address of addresses) {
    const response = await client.account.getBalance({ address });
    balances[address] = response;
  }
  
  return balances;
}

// Use batch endpoint
async function getMultipleBalancesBatched(addresses) {
  const response = await client.account.getBatchBalances({ addresses });
  return response.balances;
}
```

#### Distribute Load Over Time

Spread non-urgent requests to avoid burst patterns:

```javascript
function scheduleRequests(requests, intervalMs) {
  return new Promise((resolveAll) => {
    const results = new Array(requests.length);
    let completed = 0;
    
    requests.forEach((request, index) => {
      setTimeout(() => {
        request()
          .then(result => {
            results[index] = result;
            completed++;
            
            if (completed === requests.length) {
              resolveAll(results);
            }
          })
          .catch(error => {
            results[index] = { error };
            completed++;
            
            if (completed === requests.length) {
              resolveAll(results);
            }
          });
      }, index * intervalMs);
    });
  });
}

// Usage
const fetchFunctions = addresses.map(address => 
  () => client.account.getBalance({ address })
);

scheduleRequests(fetchFunctions, 100).then(results => {
  console.log('All balances:', results);
});
```

## Caching

### Server-Side Caching

ProzChain's API implements several caching strategies to improve performance and reduce load:

#### Time-Based Caching

Different endpoints use different cache durations based on data volatility:

| Endpoint | Cache Duration | Notes |
|----------|---------------|-------|
| `/v1/blocks/{number}` | Very long | Historical blocks don't change |
| `/v1/blocks/latest` | 2-3 seconds | Latest block updates frequently |
| `/v1/accounts/{address}/balance` | 5-10 seconds | Balances change with transactions |
| `/v1/gas/price` | 10-15 seconds | Gas prices fluctuate with network load |

#### ETag / Conditional Requests

The API supports conditional requests using ETags:

```http
// First request
GET /v1/blocks/12345 HTTP/1.1
Host: api.prozchain.com

// Response includes ETag
HTTP/1.1 200 OK
ETag: "33a64df551425fcc55e4d42a148795d9f25f89d4"
Content-Type: application/json

// Subsequent request with If-None-Match
GET /v1/blocks/12345 HTTP/1.1
Host: api.prozchain.com
If-None-Match: "33a64df551425fcc55e4d42a148795d9f25f89d4"

// Response if content hasn't changed
HTTP/1.1 304 Not Modified
ETag: "33a64df551425fcc55e4d42a148795d9f25f89d4"
```

When the server responds with `304 Not Modified`, you should use your cached copy of the data.

### Client-Side Caching

Implementing effective client-side caching can significantly improve application performance and reduce API calls.

#### Response Caching

Basic in-memory cache implementation:

```javascript
class ResponseCache {
  constructor(options = {}) {
    this.cache = new Map();
    this.defaultTtl = options.defaultTtl || 30000; // 30 seconds
    this.maxSize = options.maxSize || 100; // Maximum cache entries
  }
  
  set(key, value, ttl = this.defaultTtl) {
    // Evict old entries if cache is full
    if (this.cache.size >= this.maxSize && !this.cache.has(key)) {
      const oldest = Array.from(this.cache.keys())[0];
      this.cache.delete(oldest);
    }
    
    this.cache.set(key, {
      value,
      expiry: Date.now() + ttl,
    });
  }
  
  get(key) {
    const entry = this.cache.get(key);
    if (!entry) return null;
    
    // Check if entry is expired
    if (entry.expiry < Date.now()) {
      this.cache.delete(key);
      return null;
    }
    
    return entry.value;
  }
  
  invalidate(key) {
    this.cache.delete(key);
  }
  
  clear() {
    this.cache.clear();
  }
}

// Usage with API client
const cache = new ResponseCache();

async function getBlockCached(blockNumber) {
  const cacheKey = `block:${blockNumber}`;
  
  // Try to get from cache first
  const cachedBlock = cache.get(cacheKey);
  if (cachedBlock) {
    return cachedBlock;
  }
  
  // If not in cache, fetch from API
  const block = await client.chain.getBlock({ number: blockNumber });
  
  // Cache the result (store longer if it's not a recent block)
  const ttl = blockNumber < (await client.chain.getBlockNumber() - 100) 
    ? 3600000  // 1 hour for older blocks
    : 10000;   // 10 seconds for recent blocks
  
  cache.set(cacheKey, block, ttl);
  return block;
}
```

#### Smart Cache Invalidation

Selectively invalidate related cache entries:

```javascript
function invalidateRelatedCaches(transaction) {
  // Invalidate account balances for addresses involved in transaction
  cache.invalidate(`balance:${transaction.from}`);
  if (transaction.to) {
    cache.invalidate(`balance:${transaction.to}`);
  }
  
  // Invalidate account transaction lists
  cache.invalidate(`transactions:${transaction.from}`);
  if (transaction.to) {
    cache.invalidate(`transactions:${transaction.to}`);
  }
  
  // If contract interaction, invalidate contract state
  if (transaction.input && transaction.input !== '0x') {
    cache.invalidate(`contract:${transaction.to}`);
  }
}
```

#### Cache with WebSocket Updates

Combine cache with WebSocket updates for real-time data:

```javascript
class ReactiveCache {
  constructor(client, options = {}) {
    this.client = client;
    this.cache = new ResponseCache(options);
    this.subscriptions = new Map();
  }
  
  async initialize() {
    // Subscribe to new blocks
    this.blockSubscription = await this.client.subscribe('newBlocks');
    this.blockSubscription.on('data', this.handleNewBlock.bind(this));
    
    // Subscribe to pending transactions
    this.txSubscription = await this.client.subscribe('pendingTransactions');
    this.txSubscription.on('data', this.handlePendingTransaction.bind(this));
  }
  
  handleNewBlock(block) {
    // Update block cache
    this.cache.set(`block:${block.number}`, block, 3600000); // 1 hour TTL
    this.cache.set(`blockByHash:${block.hash}`, block, 3600000);
    
    // Invalidate "latest block" since we have a new one
    this.cache.invalidate('block:latest');
    
    // Invalidate blocks count
    this.cache.invalidate('blocks:count');
    
    // Notify subscribers
    if (this.subscriptions.has('newBlock')) {
      this.subscriptions.get('newBlock').forEach(callback => callback(block));
    }
  }
  
  // Subscribe to cache update events
  subscribe(event, callback) {
    if (!this.subscriptions.has(event)) {
      this.subscriptions.set(event, new Set());
    }
    
    this.subscriptions.get(event).add(callback);
    
    // Return unsubscribe function
    return () => {
      const callbacks = this.subscriptions.get(event);
      if (callbacks) {
        callbacks.delete(callback);
      }
    };
  }
  
  // Get data with caching
  async getData(type, id, fetcher, ttl) {
    const cacheKey = `${type}:${id}`;
    
    // Try cache first
    const cached = this.cache.get(cacheKey);
    if (cached) return cached;
    
    // Fetch data
    const data = await fetcher();
    
    // Cache result
    this.cache.set(cacheKey, data, ttl);
    
    return data;
  }
  
  // Clean up resources
  dispose() {
    if (this.blockSubscription) {
      this.blockSubscription.unsubscribe();
    }
    
    if (this.txSubscription) {
      this.txSubscription.unsubscribe();
    }
    
    this.cache.clear();
    this.subscriptions.clear();
  }
}

// Usage
const reactiveCache = new ReactiveCache(client);
await reactiveCache.initialize();

// Get data with automatic caching
const block = await reactiveCache.getData(
  'block',
  12345,
  () => client.chain.getBlock({ number: 12345 }),
  3600000 // 1 hour TTL
);

// Subscribe to updates
const unsubscribe = reactiveCache.subscribe('newBlock', (block) => {
  console.log('New block:', block.number);
  updateUI(block);
});

// Later when done
unsubscribe();
reactiveCache.dispose();
```

### Cache Headers and HTTP Caching

The API uses standard cache-control headers that your HTTP client can leverage:

```http
Cache-Control: max-age=30, public
ETag: "33a64df551425fcc55e4d42a148795d9f25f89d4"
Last-Modified: Wed, 21 Oct 2023 07:28:00 GMT
```

Configure your HTTP client to respect these headers:

```javascript
// Using fetch with caching headers
async function fetchWithCaching(url) {
  const cachedResponse = await caches.match(url);
  
  if (cachedResponse) {
    // Check if we can use cached response
    return cachedResponse;
  }
  
  // Make network request with conditional headers
  const response = await fetch(url);
  
  if (response.ok) {
    // Cache the response for future use
    const cache = await caches.open('api-cache');
    cache.put(url, response.clone());
  }
  
  return response;
}
```

## Rate Limit and Caching Strategic Considerations

### Optimizing for Different API Types

#### REST API

- Use HTTP caching headers for automatic caching
- Implement conditional requests with ETags
- Cache longer for static resources

#### RPC API

- Batch multiple method calls in a single request
- Cache method call results locally
- Use subscription methods instead of polling

#### WebSocket API

- Use event-based updates instead of frequent polling
- Reconnect with exponential backoff
- Implement message queuing for offline scenarios

#### GraphQL API

- Request only needed fields to optimize response size
- Use fragments for consistent field selection
- Combine multiple queries in a single request

### Rate Limit Monitoring

Track your usage to avoid hitting limits:

```javascript
class RateLimitTracker {
  constructor() {
    this.endpoints = new Map();
  }
  
  updateFromHeaders(endpoint, headers) {
    const limit = parseInt(headers.get('X-RateLimit-Limit') || '0', 10);
    const remaining = parseInt(headers.get('X-RateLimit-Remaining') || '0', 10);
    const reset = parseInt(headers.get('X-RateLimit-Reset') || '0', 10);
    
    this.endpoints.set(endpoint, { limit, remaining, reset });
    
    // Calculate usage percentage
    const usagePercent = ((limit - remaining) / limit) * 100;
    
    // Warn if approaching limit
    if (usagePercent > 80) {
      console.warn(`Rate limit warning: ${usagePercent.toFixed(1)}% used for ${endpoint}`);
    }
  }
  
  getStatus(endpoint) {
    return this.endpoints.get(endpoint);
  }
  
  canMakeRequest(endpoint) {
    const status = this.endpoints.get(endpoint);
    if (!status) return true; // No data yet, assume we can make the request
    
    return status.remaining > 0;
  }
  
  timeUntilReset(endpoint) {
    const status = this.endpoints.get(endpoint);
    if (!status) return 0;
    
    const now = Math.floor(Date.now() / 1000);
    return Math.max(0, status.reset - now);
  }
}

// Usage
const rateLimitTracker = new RateLimitTracker();

async function fetchWithTracking(url) {
  const endpoint = new URL(url).pathname;
  
  // Check if we have remaining requests
  if (!rateLimitTracker.canMakeRequest(endpoint)) {
    const waitTime = rateLimitTracker.timeUntilReset(endpoint);
    console.log(`Rate limit reached. Waiting ${waitTime} seconds before retry.`);
    await new Promise(resolve => setTimeout(resolve, waitTime * 1000));
  }
  
  const response = await fetch(url);
  
  // Update rate limit tracker with headers
  rateLimitTracker.updateFromHeaders(endpoint, response.headers);
  
  return response;
}
```

### Enterprise Solutions

For high-volume applications, consider these enterprise approaches:

1. **Dedicated Endpoints**: Contact sales for dedicated API endpoints with higher rate limits
2. **Cluster Deployment**: Deploy your application across multiple regions with separate API keys
3. **Proxy Layer**: Implement a proxy layer with intelligent request routing and caching
4. **Data Sync**: Pull blockchain data periodically to local database for high-read workloads

## Rate Limit Request Process

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│ Application │     │ Rate Limiter│     │  ProzChain  │     │  Database   │
│             │     │             │     │     API     │     │             │
└──────┬──────┘     └──────┬──────┘     └──────┬──────┘     └──────┬──────┘
       │                   │                   │                   │
       │ 1. API Request    │                   │                   │
       ├──────────────────►│                   │                   │
       │                   │                   │                   │
       │                   │ 2. Allowed?       │                   │
       │                   ├─────────┐         │                   │
       │                   │         │         │                   │
       │                   │◄────────┘         │                   │
       │                   │                   │                   │
       │                   │ 3. Forward Request│                   │
       │                   ├──────────────────►│                   │
       │                   │                   │                   │
       │                   │                   │ 4. Process Request│
       │                   │                   ├──────────────────►│
       │                   │                   │                   │
       │                   │                   │ 5. Database Result│
       │                   │                   │◄──────────────────┤
       │                   │                   │                   │
       │                   │ 6. API Response   │                   │
       │                   │◄──────────────────┤                   │
       │                   │                   │                   │
       │ 7. Final Response │                   │                   │
       │◄──────────────────┤                   │                   │
       │                   │                   │                   │
```

## Conclusion

Implementing effective rate limiting and caching strategies is essential for building performant and reliable applications with the ProzChain API. By following the guidelines in this document, you can:

1. Stay within rate limits and avoid disruptions to your service
2. Reduce API calls through intelligent caching
3. Improve application performance and responsiveness 
4. Provide a better user experience with real-time data updates
5. Optimize resource usage and costs

For advanced rate limiting needs or to request higher limits, please contact support@prozchain.com.

[Back to API Layer Index](./10-0-api-layer-index.md)
