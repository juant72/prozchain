# 5. Transaction Storage

## 5.1 Transaction Format
How transaction data is stored for efficiency and queryability.

```rust
struct TransactionStorage {
    db: Database,
    tx_body_store: BlobStore,
    indices: TransactionIndices,
    config: TransactionStorageConfig,
}

struct TransactionIndices {
    tx_by_hash: Database,
    tx_by_block: Database,
    tx_by_account: Database,
}

struct TransactionLocation {
    block_hash: BlockHash,
    block_height: BlockHeight,
    index: u32,
    timestamp: Timestamp,
}

struct TransactionStorageConfig {
    enable_account_index: bool,
    tx_compression_threshold: usize,
    blob_store_sharding: bool,
}

impl TransactionStorage {
    fn store_transaction(&mut self, tx: &Transaction, location: &TransactionLocation) -> Result<()> {
        let tx_hash = tx.hash();
        
        // Store transaction data
        self.store_transaction_data(&tx_hash, tx)?;
        
        // Store location indices
        self.store_transaction_indices(&tx_hash, location)?;
        
        // Store account indices if enabled
        if self.config.enable_account_index {
            self.index_transaction_by_account(tx, &tx_hash, location)?;
        }
        
        Ok(())
    }
    
    fn store_transaction_data(&mut self, tx_hash: &TransactionHash, tx: &Transaction) -> Result<()> {
        // Serialize the transaction
        let serialized = serialize(tx)?;
        
        // Compress if larger than threshold
        let data = if serialized.len() > self.config.tx_compression_threshold {
            let compressed = compress_data(&serialized)?;
            compressed
        } else {
            serialized
        };
        
        // Store in blob store
        self.tx_body_store.store(tx_hash.as_ref(), &data)?;
        
        Ok(())
    }
    
    fn store_transaction_indices(&mut self, tx_hash: &TransactionHash, location: &TransactionLocation) -> Result<()> {
        // Index by hash
        let tx_key = make_key(KeyPrefix::TransactionLocation, tx_hash);
        self.indices.tx_by_hash.put(&tx_key, &serialize(location)?)?;
        
        // Index by block
        let block_tx_key = make_key(
            KeyPrefix::BlockTransactions, 
            &(location.block_hash, location.index)
        );
        self.indices.tx_by_block.put(&block_tx_key, tx_hash.as_ref())?;
        
        Ok(())
    }
    
    fn index_transaction_by_account(&mut self, tx: &Transaction, tx_hash: &TransactionHash, location: &TransactionLocation) -> Result<()> {
        // Index by sender
        let sender = tx.sender();
        let sender_key = make_key(
            KeyPrefix::AccountTransactions, 
            &(sender, location.timestamp)
        );
        self.indices.tx_by_account.put(&sender_key, tx_hash.as_ref())?;
        
        // Index by receiver if it's a regular transfer
        if let Some(receiver) = tx.receiver() {
            let receiver_key = make_key(
                KeyPrefix::AccountTransactions, 
                &(receiver, location.timestamp)
            );
            self.indices.tx_by_account.put(&receiver_key, tx_hash.as_ref())?;
        }
        
        Ok(())
    }
    
    fn get_transaction(&self, hash: &TransactionHash) -> Result<Option<Transaction>> {
        // Get transaction location
        let tx_key = make_key(KeyPrefix::TransactionLocation, hash);
        let location_data = match self.indices.tx_by_hash.get(&tx_key)? {
            Some(data) => data,
            None => return Ok(None),
        };
        
        // Deserialize location
        let _location: TransactionLocation = deserialize(&location_data)?;
        
        // Get transaction data from blob store
        let data = self.tx_body_store.get(hash.as_ref())?;
        
        // Decompress if needed and deserialize
        let tx: Transaction = if is_compressed(&data) {
            let decompressed = decompress_data(&data)?;
            deserialize(&decompressed)?
        } else {
            deserialize(&data)?
        };
        
        Ok(Some(tx))
    }
    
    fn get_transactions_by_block(&self, block_hash: &BlockHash) -> Result<Vec<Transaction>> {
        // Get block metadata to know how many transactions
        let tx_count = self.get_block_transaction_count(block_hash)?;
        let mut transactions = Vec::with_capacity(tx_count);
        
        // Retrieve each transaction by index
        for index in 0..tx_count {
            let block_tx_key = make_key(
                KeyPrefix::BlockTransactions, 
                &(block_hash, index as u32)
            );
            
            if let Some(hash_bytes) = self.indices.tx_by_block.get(&block_tx_key)? {
                let tx_hash = TransactionHash::from_slice(&hash_bytes)?;
                if let Some(tx) = self.get_transaction(&tx_hash)? {
                    transactions.push(tx);
                }
            }
        }
        
        Ok(transactions)
    }
}
```

**Design Rationale**:
- **Separation of Concerns**: Transaction data and indices stored separately
- **Compression**: Saves space for large transactions
- **Flexible Indexing**: Multiple access paths for different query patterns
- **Blob Storage**: Optimized for large binary objects

**For Beginners**: Transaction storage is like an organized filing system where the raw transaction data is stored efficiently in one place, but there are multiple cross-reference indices that let you look up transactions by different criteria, like their hash, which block they're in, or which accounts they involve.

## 5.2 Receipt Storage
Stores transaction execution results for later reference and verification.

```rust
struct ReceiptStore {
    db: Database,
    event_index: EventIndex,
    config: ReceiptStorageConfig,
}

struct Receipt {
    transaction_hash: TransactionHash,
    status: ExecutionStatus,
    gas_used: u64,
    logs: Vec<Log>,
    return_data: Option<Vec<u8>>,
}

struct EventIndex {
    db: Database,
    bloom_filters: HashMap<BlockHeight, LogBloomFilter>,
}

struct ReceiptStorageConfig {
    enable_bloom_filters: bool,
    index_event_topics: bool,
}

impl ReceiptStore {
    fn store_receipt(&mut self, receipt: &Receipt, block_height: BlockHeight) -> Result<()> {
        let tx_hash = receipt.transaction_hash;
        
        // Store receipt
        let receipt_key = make_key(KeyPrefix::Receipt, &tx_hash);
        self.db.put(&receipt_key, &serialize(receipt)?)?;
        
        // Index events if configured
        if self.config.index_event_topics {
            self.index_events(receipt, block_height)?;
        }
        
        // Update bloom filter
        if self.config.enable_bloom_filters {
            self.update_bloom_filter(receipt, block_height)?;
        }
        
        Ok(())
    }
    
    fn index_events(&mut self, receipt: &Receipt, block_height: BlockHeight) -> Result<()> {
        // For each log in the receipt
        for (log_index, log) in receipt.logs.iter().enumerate() {
            // Index by address
            let address_key = make_key(
                KeyPrefix::EventsByAddress, 
                &(log.address, block_height, receipt.transaction_hash, log_index as u32)
            );
            self.event_index.db.put(&address_key, &[])?; // Value not needed, key encodes all info
            
            // Index by each topic
            for (topic_index, topic) in log.topics.iter().enumerate() {
                let topic_key = make_key(
                    KeyPrefix::EventsByTopic, 
                    &(*topic, block_height, receipt.transaction_hash, log_index as u32, topic_index as u8)
                );
                self.event_index.db.put(&topic_key, &[])?;
            }
        }
        
        Ok(())
    }
    
    fn update_bloom_filter(&mut self, receipt: &Receipt, block_height: BlockHeight) -> Result<()> {
        // Get or create bloom filter for this block
        let bloom = self.event_index.bloom_filters
            .entry(block_height)
            .or_insert_with(|| LogBloomFilter::new());
        
        // Add all addresses and topics to the bloom filter
        for log in &receipt.logs {
            bloom.add(&log.address);
            
            for topic in &log.topics {
                bloom.add(topic);
            }
        }
        
        // Store updated bloom filter
        let bloom_key = make_key(KeyPrefix::BlockBloom, &block_height);
        self.db.put(&bloom_key, &serialize(bloom)?)?;
        
        Ok(())
    }
    
    fn get_receipt(&self, tx_hash: &TransactionHash) -> Result<Option<Receipt>> {
        let receipt_key = make_key(KeyPrefix::Receipt, tx_hash);
        
        match self.db.get(&receipt_key)? {
            Some(data) => {
                let receipt: Receipt = deserialize(&data)?;
                Ok(Some(receipt))
            }
            None => Ok(None),
        }
    }
    
    fn query_logs(&self, filter: &LogFilter) -> Result<Vec<FilteredLog>> {
        let mut results = Vec::new();
        
        // Check if we can use bloom filters to optimize the search
        if self.config.enable_bloom_filters {
            // Find candidate blocks using bloom filters
            let candidate_blocks = self.find_candidate_blocks(filter)?;
            
            // Process each candidate block
            for block_height in candidate_blocks {
                self.collect_logs_from_block(block_height, filter, &mut results)?;
            }
        } else {
            // Fallback to scanning the direct indices
            self.scan_indices_for_logs(filter, &mut results)?;
        }
        
        Ok(results)
    }
    
    fn find_candidate_blocks(&self, filter: &LogFilter) -> Result<Vec<BlockHeight>> {
        // Use bloom filters to quickly eliminate blocks that definitely don't contain matching logs
        let mut candidates = Vec::new();
        
        // Range of blocks to check
        let start_height = filter.from_block.unwrap_or(0);
        let end_height = filter.to_block.unwrap_or(self.get_latest_block_height()?);
        
        for height in start_height..=end_height {
            let bloom_key = make_key(KeyPrefix::BlockBloom, &height);
            
            if let Some(bloom_data) = self.db.get(&bloom_key)? {
                let bloom: LogBloomFilter = deserialize(&bloom_data)?;
                
                // Check if this block might contain relevant logs
                let mut might_match = true;
                
                // Check addresses filter
                if let Some(addresses) = &filter.addresses {
                    might_match &= addresses.iter().any(|addr| bloom.might_contain(addr));
                }
                
                // Check topic filters
                if might_match && !filter.topics.is_empty() {
                    for topic_set in &filter.topics {
                        match topic_set {
                            // Must match any of these topics
                            TopicFilter::Any(topics) if !topics.is_empty() => {
                                might_match &= topics.iter().any(|topic| bloom.might_contain(topic));
                            },
                            // Must match this specific topic
                            TopicFilter::Single(topic) => {
                                might_match &= bloom.might_contain(topic);
                            },
                            // No topics or any topics are allowed
                            _ => {}
                        }
                        
                        if !might_match {
                            break;
                        }
                    }
                }
                
                if might_match {
                    candidates.push(height);
                }
            }
        }
        
        Ok(candidates)
    }
}
```

**Design Rationale**:
- **Complete Record**: Stores all execution results and events
- **Bloom Filtering**: Quick elimination of irrelevant blocks in queries
- **Flexible Querying**: Multiple indices for different query patterns
- **Space-Efficient**: Minimizes duplication and overhead

**For Beginners**: Receipt storage is like keeping detailed records of each transaction's outcomes and effects. It allows efficient searching through these records, using techniques like bloom filters (which are like quick-check lists that tell you if something might be in a certain block) to speed up queries.

## 5.3 Event Indexing
Specialized indexing for smart contract events/logs.

```rust
struct EventIndexer {
    db: Database,
    topic_indices: HashMap<H256, TopicIndex>,
    address_index: AddressIndex,
    block_range: RangeIndex<BlockHeight>,
}

struct TopicIndex {
    db: Database,
    topic: H256,
    entries: BTreeMap<BlockHeight, Vec<EventEntry>>,
}

struct AddressIndex {
    db: Database,
    entries: HashMap<Address, BTreeMap<BlockHeight, Vec<EventEntry>>>,
}

struct RangeIndex<T: Ord> {
    ranges: BTreeMap<T, Vec<EventEntry>>,
}

struct EventEntry {
    block_height: BlockHeight,
    tx_hash: TransactionHash,
    log_index: u32,
    address: Address,
    topics: Vec<H256>,
    data: Vec<u8>,
}

impl EventIndexer {
    fn index_block_events(&mut self, block: &Block, receipts: &[Receipt]) -> Result<()> {
        let block_height = block.header.height;
        let mut batch = WriteBatch::new();
        
        for (tx_index, receipt) in receipts.iter().enumerate() {
            for (log_index, log) in receipt.logs.iter().enumerate() {
                let entry = EventEntry {
                    block_height,
                    tx_hash: receipt.transaction_hash,
                    log_index: log_index as u32,
                    address: log.address,
                    topics: log.topics.clone(),
                    data: log.data.clone(),
                };
                
                // Index by address
                self.index_by_address(&mut batch, &entry)?;
                
                // Index by topics
                self.index_by_topics(&mut batch, &entry)?;
                
                // Index by block range
                self.index_by_block_range(&mut batch, &entry)?;
            }
        }
        
        // Commit all indices at once
        self.db.write(batch)?;
        
        Ok(())
    }
    
    fn index_by_address(&self, batch: &mut WriteBatch, entry: &EventEntry) -> Result<()> {
        // Create composite key: address + block_height + tx_hash + log_index
        let key = make_key(
            KeyPrefix::EventsByAddress, 
            &(entry.address, entry.block_height, entry.tx_hash, entry.log_index)
        );
        
        // Store the event data
        batch.put(key, &serialize(entry)?);
        
        Ok(())
    }
    
    fn index_by_topics(&self, batch: &mut WriteBatch, entry: &EventEntry) -> Result<()> {
        // Index each topic separately
        for (topic_index, topic) in entry.topics.iter().enumerate() {
            let key = make_key(
                KeyPrefix::EventsByTopic,
                &(*topic, entry.block_height, entry.tx_hash, entry.log_index, topic_index as u8)
            );
            
            // Just store a reference, not the full entry
            batch.put(key, &[]);
        }
        
        Ok(())
    }
    
    fn index_by_block_range(&self, batch: &mut WriteBatch, entry: &EventEntry) -> Result<()> {
        // Create a key that can be queried by block range
        let key = make_key(
            KeyPrefix::EventsByBlock,
            &(entry.block_height, entry.tx_hash, entry.log_index)
        );
        
        // Store address and topics for filtering
        let filter_data = EventFilterData {
            address: entry.address,
            topics: entry.topics.clone(),
        };
        
        batch.put(key, &serialize(&filter_data)?);
        
        Ok(())
    }
    
    fn query_events(&self, filter: &EventFilter) -> Result<Vec<EventEntry>> {
        let mut results = Vec::new();
        
        // Determine most efficient query strategy
        if let Some(address) = &filter.address {
            // Query by address first
            self.query_by_address(address, filter, &mut results)?;
        } else if !filter.topics.is_empty() {
            // Query by topic
            self.query_by_topics(&filter.topics[0], filter, &mut results)?;
        } else {
            // No specific filter, use block range
            self.query_by_block_range(filter, &mut results)?;
        }
        
        Ok(results)
    }
    
    fn query_by_address(&self, address: &Address, filter: &EventFilter, results: &mut Vec<EventEntry>) -> Result<()> {
        // Create prefix for all events from this address
        let prefix = make_key(KeyPrefix::EventsByAddress, address);
        
        // Iterate matching keys
        let iter = self.db.iter_prefix(&prefix)?;
        for item in iter {
            let (_, value) = item?;
            let entry: EventEntry = deserialize(&value)?;
            
            // Apply additional filters
            if self.matches_filter(&entry, filter) {
                results.push(entry);
            }
        }
        
        Ok(())
    }
    
    fn matches_filter(&self, entry: &EventEntry, filter: &EventFilter) -> bool {
        // Check block range
        if let Some(from_block) = filter.from_block {
            if entry.block_height < from_block {
                return false;
            }
        }
        
        if let Some(to_block) = filter.to_block {
            if entry.block_height > to_block {
                return false;
            }
        }
        
        // Check topics
        for (i, topic_filter) in filter.topics.iter().enumerate() {
            if i >= entry.topics.len() {
                // Entry has fewer topics than filter specifies
                return false;
            }
            
            match topic_filter {
                TopicFilter::Any(allowed_topics) => {
                    if !allowed_topics.contains(&entry.topics[i]) {
                        return false;
                    }
                },
                TopicFilter::Single(required_topic) => {
                    if &entry.topics[i] != required_topic {
                        return false;
                    }
                },
                TopicFilter::None => {}
            }
        }
        
        true
    }
}
```

**Design Rationale**:
- **Multi-dimensional Indexing**: Events can be queried by multiple criteria
- **Query Optimization**: Different lookup strategies for different filter types
- **Batch Processing**: Efficient bulk updates during block processing
- **Composite Keys**: Clever key construction for range queries

**For Beginners**: Event indexing is like creating special catalogs for events in the blockchain - you can look up events by address (who generated them), topics (what they're about), or block range (when they happened). This makes it efficient to find specific events among millions.

[Back to Index](./03-0-storage-layer-index.md) | [Previous: State Database](./03.04-storage-layer-state-database.md) | [Next: Pruning Mechanisms](./03.06-storage-layer-pruning.md)
