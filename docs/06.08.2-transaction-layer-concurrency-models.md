# 8.2 Concurrency Models

Concurrency models define how transactions are executed in parallel while respecting their dependencies. ProzChain implements multiple concurrency models to adapt to different scenarios and transaction types.

## Concurrency Model Types

```rust
/// Defines strategies for parallel transaction execution
enum ConcurrencyModel {
    /// No concurrency, transactions executed sequentially
    Sequential,
    
    /// Transactions executed in parallel based on dependency analysis
    DependencyBased {
        max_threads: usize,
    },
    
    /// Transactions executed speculatively with retry on conflicts
    Optimistic {
        max_threads: usize,
        max_retries: usize,
    },
    
    /// Split transactions into shards for parallel execution
    Sharded {
        shard_count: usize,
    },
    
    /// Dynamic approach that adapts based on transaction characteristics
    Adaptive {
        strategies: Vec<ConcurrencyModel>,
        selection_policy: SelectionPolicy,
    },
}

/// Policy for selecting which concurrency model to use
enum SelectionPolicy {
    /// Based on transaction load
    LoadBased {
        low_threshold: usize,
        high_threshold: usize,
    },
    
    /// Based on transaction characteristics
    CharacteristicBased,
    
    /// Based on historical performance
    PerformanceBased {
        window_size: usize,
    },
    
    /// Custom selection logic
    Custom(Box<dyn Fn(&[Transaction]) -> ConcurrencyModel + Send + Sync>),
}
```

## Parallel Executor

```rust
/// Executor for parallel transaction processing
struct ParallelExecutor {
    /// Underlying state manager
    state_manager: Arc<StateManager>,
    
    /// Execution environment for transactions
    vm: Arc<ExecutionEnvironment>,
    
    /// Dependency analyzer
    dependency_analyzer: Arc<DependencyAnalyzer>,
    
    /// Concurrency model to use
    concurrency_model: ConcurrencyModel,
    
    /// Thread pool for parallel execution
    thread_pool: ThreadPool,
    
    /// Statistics and metrics
    metrics: ExecutorMetrics,
}

/// Tracks metrics for the parallel executor
struct ExecutorMetrics {
    executed_transactions: AtomicU64,
    execution_conflicts: AtomicU64,
    total_execution_time_us: AtomicU64,
    max_parallel_transactions: AtomicU64,
}

impl ParallelExecutor {
    fn new(
        state_manager: Arc<StateManager>,
        vm: Arc<ExecutionEnvironment>,
        dependency_analyzer: Arc<DependencyAnalyzer>,
        concurrency_model: ConcurrencyModel,
    ) -> Self {
        // Create thread pool based on concurrency model
        let pool_size = match &concurrency_model {
            ConcurrencyModel::Sequential => 1,
            ConcurrencyModel::DependencyBased { max_threads } => *max_threads,
            ConcurrencyModel::Optimistic { max_threads, .. } => *max_threads,
            ConcurrencyModel::Sharded { shard_count } => *shard_count,
            ConcurrencyModel::Adaptive { .. } => num_cpus::get(),
        };
        
        let thread_pool = ThreadPool::new(pool_size);
        
        ParallelExecutor {
            state_manager,
            vm,
            dependency_analyzer,
            concurrency_model,
            thread_pool,
            metrics: ExecutorMetrics {
                executed_transactions: AtomicU64::new(0),
                execution_conflicts: AtomicU64::new(0),
                total_execution_time_us: AtomicU64::new(0),
                max_parallel_transactions: AtomicU64::new(0),
            },
        }
    }
    
    /// Execute a batch of transactions in parallel based on the concurrency model
    fn execute_transactions(&self, transactions: Vec<Transaction>) -> Result<Vec<TransactionResult>, Error> {
        let start_time = Instant::now();
        
        let results = match &self.concurrency_model {
            ConcurrencyModel::Sequential => {
                self.execute_sequential(&transactions)?
            },
            ConcurrencyModel::DependencyBased { .. } => {
                self.execute_dependency_based(&transactions)?
            },
            ConcurrencyModel::Optimistic { max_retries, .. } => {
                self.execute_optimistic(&transactions, *max_retries)?
            },
            ConcurrencyModel::Sharded { shard_count } => {
                self.execute_sharded(&transactions, *shard_count)?
            },
            ConcurrencyModel::Adaptive { strategies, selection_policy } => {
                // Select the best strategy based on the policy
                let selected_model = match selection_policy {
                    SelectionPolicy::LoadBased { low_threshold, high_threshold } => {
                        self.select_load_based_model(strategies, *low_threshold, *high_threshold, transactions.len())
                    },
                    SelectionPolicy::CharacteristicBased => {
                        self.select_characteristic_based_model(strategies, &transactions)
                    },
                    SelectionPolicy::PerformanceBased { window_size } => {
                        self.select_performance_based_model(strategies, *window_size)
                    },
                    SelectionPolicy::Custom(selector) => {
                        selector(&transactions)
                    },
                };
                
                // Create a temporary executor with the selected model
                let temp_executor = ParallelExecutor::new(
                    self.state_manager.clone(),
                    self.vm.clone(),
                    self.dependency_analyzer.clone(),
                    selected_model,
                );
                
                temp_executor.execute_transactions(transactions)?
            }
        };
        
        // Update metrics
        let elapsed_us = start_time.elapsed().as_micros() as u64;
        self.metrics.executed_transactions.fetch_add(transactions.len() as u64, Ordering::Relaxed);
        self.metrics.total_execution_time_us.fetch_add(elapsed_us, Ordering::Relaxed);
        
        Ok(results)
    }
    
    // Implementation for different concurrency models follows
}
```

## Sequential Execution

The simplest execution mode processes transactions one after another:

```rust
impl ParallelExecutor {
    /// Execute transactions sequentially
    fn execute_sequential(&self, transactions: &[Transaction]) -> Result<Vec<TransactionResult>, Error> {
        let mut results = Vec::with_capacity(transactions.len());
        let mut state = self.state_manager.get_mutable_state()?;
        
        for tx in transactions {
            let result = self.vm.execute_transaction(&mut state, tx)?;
            results.push(result);
        }
        
        Ok(results)
    }
}
```

## Dependency-Based Execution

This model uses transaction dependency analysis to execute independent transactions in parallel:

```rust
impl ParallelExecutor {
    /// Execute transactions in parallel based on dependency analysis
    fn execute_dependency_based(&self, transactions: &[Transaction]) -> Result<Vec<TransactionResult>, Error> {
        // Step 1: Analyze dependencies
        let dependency_graph = self.dependency_analyzer.analyze_dependencies(transactions)?;
        
        // Step 2: Identify independent transaction groups that can be executed in parallel
        let execution_groups = dependency_graph.identify_parallel_groups();
        
        // Step 3: Execute each group in parallel
        let mut all_results = vec![None; transactions.len()];
        
        // Map of transaction hash to original index
        let tx_map: HashMap<_, _> = transactions.iter()
            .enumerate()
            .map(|(i, tx)| (tx.hash, i))
            .collect();
        
        // Get initial state
        let base_state = self.state_manager.get_current_state()?;
        
        // Execute each group of independent transactions
        for group in execution_groups {
            let group_len = group.len();
            let mut handles = Vec::with_capacity(group_len);
            
            // Create thread-safe result collection
            let results = Arc::new(Mutex::new(HashMap::with_capacity(group_len)));
            
            // Execute transactions in this group in parallel
            for tx_hash in group {
                let tx_idx = *tx_map.get(&tx_hash).unwrap();
                let tx = transactions[tx_idx].clone();
                let state_copy = base_state.clone();
                let vm = self.vm.clone();
                let results = results.clone();
                
                let handle = self.thread_pool.spawn(move || {
                    let mut local_state = state_copy;
                    let result = vm.execute_transaction(&mut local_state, &tx)?;
                    results.lock().unwrap().insert(tx_idx, result);
                    Ok::<_, Error>(())
                });
                
                handles.push(handle);
            }
            
            // Wait for all tasks to complete
            for handle in handles {
                handle.join().unwrap()?;
            }
            
            // Copy results to the output vector in the correct order
            let result_map = results.lock().unwrap();
            for (&idx, result) in result_map.iter() {
                all_results[idx] = Some(result.clone());
            }
        }
        
        // Unwrap all results
        Ok(all_results.into_iter().map(|r| r.unwrap()).collect())
    }
}
```

## Optimistic Execution

The optimistic model executes transactions in parallel and retries any that experience conflicts:

```rust
impl ParallelExecutor {
    /// Execute transactions optimistically, retrying on conflicts
    fn execute_optimistic(&self, transactions: &[Transaction], max_retries: usize) -> Result<Vec<TransactionResult>, Error> {
        // Initialize results array and conflict tracking
        let mut results = vec![None; transactions.len()];
        let mut conflicts = HashSet::new();
        let mut retry_count = 0;
        
        // Get base state
        let base_state = self.state_manager.get_current_state()?;
        
        // Keep track of transactions that need execution
        let mut to_execute: Vec<_> = (0..transactions.len()).collect();
        
        // Execute until all transactions are processed or max retries reached
        while !to_execute.is_empty() && retry_count < max_retries {
            // Create shared state for this batch
            let shared_state = Arc::new(RwLock::new(base_state.clone()));
            let tracked_accesses = Arc::new(RwLock::new(HashMap::new()));
            
            // Execute all pending transactions
            let mut handles = Vec::with_capacity(to_execute.len());
            let current_conflicts = Arc::new(Mutex::new(HashSet::new()));
            
            for &tx_idx in &to_execute {
                let tx = transactions[tx_idx].clone();
                let state = shared_state.clone();
                let accesses = tracked_accesses.clone();
                let conflicts = current_conflicts.clone();
                let vm = self.vm.clone();
                
                let handle = self.thread_pool.spawn(move || {
                    // Execute transaction with conflict detection
                    let result = {
                        let mut state = state.write().unwrap();
                        let mut tracker = StateAccessTracker::new(&mut state);
                        let result = vm.execute_transaction(&mut tracker, &tx)?;
                        
                        // Record accesses
                        let mut accesses_map = accesses.write().unwrap();
                        accesses_map.insert(tx_idx, tracker.get_accesses());
                        
                        result
                    };
                    
                    // Return result
                    Ok::<_, Error>((tx_idx, result))
                });
                
                handles.push(handle);
            }
            
            // Collect results and detect conflicts
            for handle in handles {
                if let Ok(Ok((tx_idx, result))) = handle.join() {
                    results[tx_idx] = Some(result);
                }
            }
            
            // Check for conflicts between transactions
            let accesses_map = tracked_accesses.read().unwrap();
            
            // Simplified conflict detection - in real implementation this would be more sophisticated
            // to detect read-write conflicts accurately
            for &i in &to_execute {
                for &j in &to_execute {
                    if i == j { continue; }
                    
                    if let (Some(access_i), Some(access_j)) = (accesses_map.get(&i), accesses_map.get(&j)) {
                        // Check for conflicts
                        if access_i.conflicts_with(access_j) {
                            current_conflicts.lock().unwrap().insert(i);
                            current_conflicts.lock().unwrap().insert(j);
                            
                            // Update metrics
                            self.metrics.execution_conflicts.fetch_add(1, Ordering::Relaxed);
                        }
                    }
                }
            }
            
            // Update conflicts and prepare for retry
            let current_conflicts = Arc::try_unwrap(current_conflicts)
                .unwrap()
                .into_inner()
                .unwrap();
            
            conflicts = current_conflicts.clone();
            to_execute = conflicts.into_iter().collect();
            retry_count += 1;
        }
        
        // Check if any transactions still have conflicts after max retries
        if !to_execute.is_empty() {
            return Err(Error::OptimisticExecutionFailed);
        }
        
        // Unwrap all results
        Ok(results.into_iter().map(|r| r.unwrap()).collect())
    }
}

/// Helper for tracking state accesses during transaction execution
struct StateAccessTracker<'a> {
    state: &'a mut MutableState,
    reads: HashSet<Resource>,
    writes: HashSet<Resource>,
}

impl<'a> StateAccessTracker<'a> {
    fn new(state: &'a mut MutableState) -> Self {
        Self {
            state,
            reads: HashSet::new(),
            writes: HashSet::new(),
        }
    }
    
    fn get_accesses(&self) -> AccessSet {
        AccessSet {
            reads: self.reads.clone(),
            writes: self.writes.clone(),
        }
    }
    
    // State access methods that track reads and writes
    // These would delegate to the underlying state while recording accesses
}

impl AccessSet {
    fn conflicts_with(&self, other: &AccessSet) -> bool {
        // Read-write conflict
        for read in &self.reads {
            if other.writes.contains(read) {
                return true;
            }
        }
        
        // Write-read conflict
        for write in &self.writes {
            if other.reads.contains(write) {
                return true;
            }
        }
        
        // Write-write conflict
        for write in &self.writes {
            if other.writes.contains(write) {
                return true;
            }
        }
        
        false
    }
}
```

## Sharded Execution

Sharded execution divides transactions by account/resource affinity:

```rust
impl ParallelExecutor {
    /// Execute transactions using sharding approach
    fn execute_sharded(&self, transactions: &[Transaction], shard_count: usize) -> Result<Vec<TransactionResult>, Error> {
        // Step 1: Assign transactions to shards
        let shards = self.assign_transactions_to_shards(transactions, shard_count);
        
        // Step 2: Process each shard in parallel
        let mut all_results = vec![None; transactions.len()];
        let mut handles = Vec::with_capacity(shard_count);
        
        for shard_idx in 0..shard_count {
            // Skip empty shards
            if shards[shard_idx].is_empty() {
                continue;
            }
            
            let shard_txs = shards[shard_idx].clone();
            let vm = self.vm.clone();
            let state_manager = self.state_manager.clone();
            
            let handle = self.thread_pool.spawn(move || -> Result<HashMap<usize, TransactionResult>, Error> {
                let mut results = HashMap::new();
                let mut state = state_manager.get_mutable_state()?;
                
                // Process transactions in this shard sequentially
                for (tx_idx, tx) in shard_txs {
                    let result = vm.execute_transaction(&mut state, &tx)?;
                    results.insert(tx_idx, result);
                }
                
                Ok(results)
            });
            
            handles.push(handle);
        }
        
        // Collect results from all shards
        for handle in handles {
            match handle.join() {
                Ok(Ok(shard_results)) => {
                    for (idx, result) in shard_results {
                        all_results[idx] = Some(result);
                    }
                },
                Ok(Err(e)) => return Err(e),
                Err(_) => return Err(Error::ThreadPanicked),
            }
        }
        
        // Make sure all transactions were executed
        if all_results.iter().any(|r| r.is_none()) {
            return Err(Error::IncompleteExecution);
        }
        
        Ok(all_results.into_iter().map(|r| r.unwrap()).collect())
    }
    
    /// Assign transactions to shards based on account affinity
    fn assign_transactions_to_shards(&self, transactions: &[Transaction], shard_count: usize) -> Vec<Vec<(usize, Transaction)>> {
        let mut shards = vec![Vec::new(); shard_count];
        
        for (i, tx) in transactions.iter().enumerate() {
            let shard = self.calculate_shard_for_transaction(tx, shard_count);
            shards[shard].push((i, tx.clone()));
        }
        
        shards
    }
    
    /// Calculate which shard a transaction belongs to
    fn calculate_shard_for_transaction(&self, tx: &Transaction, shard_count: usize) -> usize {
        // Use sender address to determine shard
        // This ensures all transactions from the same account go to the same shard
        let sender_bytes = tx.sender.as_bytes();
        let first_byte = sender_bytes[0] as usize;
        first_byte % shard_count
    }
}
```

## Adaptive Execution

Adaptive execution selects the most appropriate concurrency model based on workload characteristics:

```rust
impl ParallelExecutor {
    /// Select appropriate model based on transaction load
    fn select_load_based_model(&self, strategies: &[ConcurrencyModel], low_threshold: usize, high_threshold: usize, tx_count: usize) -> &ConcurrencyModel {
        if tx_count < low_threshold {
            // Low load, use sequential execution
            strategies.iter()
                .find(|s| matches!(s, ConcurrencyModel::Sequential))
                .unwrap_or(&strategies[0])
        } else if tx_count > high_threshold {
            // High load, use sharded execution
            strategies.iter()
                .find(|s| matches!(s, ConcurrencyModel::Sharded { .. }))
                .unwrap_or(&strategies[0])
        } else {
            // Medium load, use dependency-based execution
            strategies.iter()
                .find(|s| matches!(s, ConcurrencyModel::DependencyBased { .. }))
                .unwrap_or(&strategies[0])
        }
    }
    
    /// Select appropriate model based on transaction characteristics
    fn select_characteristic_based_model(&self, strategies: &[ConcurrencyModel], transactions: &[Transaction]) -> &ConcurrencyModel {
        // Analyze transaction characteristics
        let mut contract_call_count = 0;
        let mut transfer_count = 0;
        let mut sender_set = HashSet::new();
        
        for tx in transactions {
            match tx.transaction_type {
                TransactionType::ContractCall => contract_call_count += 1,
                TransactionType::Transfer => transfer_count += 1,
                _ => {}
            }
            
            sender_set.insert(tx.sender);
        }
        
        let unique_sender_ratio = sender_set.len() as f64 / transactions.len().max(1) as f64;
        
        // Choose model based on characteristics
        if contract_call_count > transfer_count && unique_sender_ratio > 0.8 {
            // Many contract calls from different senders - optimistic might work well
            strategies.iter()
                .find(|s| matches!(s, ConcurrencyModel::Optimistic { .. }))
                .unwrap_or(&strategies[0])
        } else if unique_sender_ratio < 0.2 {
            // Few unique senders - dependency-based execution works well
            strategies.iter()
                .find(|s| matches!(s, ConcurrencyModel::DependencyBased { .. }))
                .unwrap_or(&strategies[0])
        } else {
            // Mixed workload - sharded execution works well
            strategies.iter()
                .find(|s| matches!(s, ConcurrencyModel::Sharded { .. }))
                .unwrap_or(&strategies[0])
        }
    }
    
    /// Select appropriate model based on historical performance
    fn select_performance_based_model(&self, strategies: &[ConcurrencyModel], window_size: usize) -> &ConcurrencyModel {
        // In a real implementation, this would track performance over time
        // For this example, we'll just return the first strategy
        &strategies[0]
    }
}
```

**Design Rationale**:
- **Multiple Models**: Different approaches for different transaction patterns
- **Conflict Resolution**: Mechanisms to handle conflicting transactions
- **Adaptive Selection**: Intelligence to choose the right model for the workload
- **Performance Tracking**: Metrics to evaluate each model's effectiveness

**For Beginners**: Concurrency models are like different strategies for getting work done in parallel. The "Sequential" model is like having one worker do everything in order. "Dependency-Based" is like having multiple workers who coordinate on who does what based on task dependencies. "Optimistic" is like having everyone work without coordination but fixing any conflicts that arise afterwards. "Sharded" is like dividing work by customer/account and having dedicated teams for each group. The system can automatically choose which approach works best for a given set of transactions.

[Back to Chapter 8](./06.08-transaction-layer-parallelization.md) | [Previous: Dependency Analysis](./06.08.1-transaction-layer-dependency-analysis.md) | [Next: Execution Scheduling](./06.08.3-transaction-layer-execution-scheduling.md)
