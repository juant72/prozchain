# 8. Performance Optimization

## 8.1 Caching Strategies
Techniques for using memory efficiently to speed up frequent operations.

```rust
struct CacheManager {
    block_cache: LruCache<BlockHash, Block>,
    header_cache: LruCache<BlockHash, BlockHeader>,
    state_cache: TrieNodeCache,
    transaction_cache: LruCache<TransactionHash, Transaction>,
    receipt_cache: LruCache<TransactionHash, Receipt>,
    metrics: CacheMetrics,
}

struct TrieNodeCache {
    nodes: LruCache<NodeHash, TrieNode>,
    account_values: LruCache<Address, Account>,
    storage_values: LruCache<(Address, StorageKey), StorageValue>,
}

struct CacheMetrics {
    hits: HashMap<CacheType, Counter>,
    misses: HashMap<CacheType, Counter>,
    evictions: HashMap<CacheType, Counter>,
    sizes: HashMap<CacheType, Gauge>,
    memory_usage: Gauge,
}

enum CacheType {
    Block,
    Header,
    StateNode,
    Account,
    Storage,
    Transaction,
    Receipt,
}

impl CacheManager {
    fn new(config: CacheConfig) -> Self {
        let block_cache = LruCache::new(config.block_cache_size);
        let header_cache = LruCache::new(config.header_cache_size);
        
        let state_cache = TrieNodeCache {
            nodes: LruCache::new(config.state_node_cache_size),
            account_values: LruCache::new(config.account_cache_size),
            storage_values: LruCache::new(config.storage_cache_size),
        };
        
        let transaction_cache = LruCache::new(config.transaction_cache_size);
        let receipt_cache = LruCache::new(config.receipt_cache_size);
        
        // Initialize metrics
        let mut hits = HashMap::new();
        let mut misses = HashMap::new();
        let mut evictions = HashMap::new();
        let mut sizes = HashMap::new();
        
        for cache_type in &[
            CacheType::Block,
            CacheType::Header,
            CacheType::StateNode,
            CacheType::Account,
            CacheType::Storage,
            CacheType::Transaction,
            CacheType::Receipt,
        ] {
            let type_name = format!("{:?}", cache_type).to_lowercase();
            
            hits.insert(
                cache_type.clone(),
                Counter::new(&format!("cache_{}_hits_total", type_name), 
                           &format!("{:?} cache hits", cache_type)).unwrap()
            );
            
            misses.insert(
                cache_type.clone(),
                Counter::new(&format!("cache_{}_misses_total", type_name), 
                           &format!("{:?} cache misses", cache_type)).unwrap()
            );
            
            evictions.insert(
                cache_type.clone(),
                Counter::new(&format!("cache_{}_evictions_total", type_name), 
                           &format!("{:?} cache evictions", cache_type)).unwrap()
            );
            
            sizes.insert(
                cache_type.clone(),
                Gauge::new(&format!("cache_{}_size", type_name), 
                         &format!("Current {:?} cache size", cache_type)).unwrap()
            );
        }
        
        let memory_usage = Gauge::new("cache_memory_usage_bytes", 
                                   "Total memory used by caches in bytes").unwrap();
        
        let metrics = CacheMetrics {
            hits,
            misses,
            evictions,
            sizes,
            memory_usage,
        };
        
        CacheManager {
            block_cache,
            header_cache,
            state_cache,
            transaction_cache,
            receipt_cache,
            metrics,
        }
    }
    
    fn get_block(&mut self, hash: &BlockHash, db_get: impl FnOnce() -> Result<Block>) -> Result<Block> {
        // Try cache first
        if let Some(block) = self.block_cache.get(hash) {
            self.metrics.hits.get(&CacheType::Block).unwrap().inc();
            return Ok(block.clone());
        }
        
        // Cache miss, get from database
        self.metrics.misses.get(&CacheType::Block).unwrap().inc();
        let block = db_get()?;
        
        // Update cache
        self.block_cache.put(*hash, block.clone());
        self.update_cache_metrics();
        
        Ok(block)
    }
    
    fn get_trie_node(&mut self, hash: &NodeHash, db_get: impl FnOnce() -> Result<TrieNode>) -> Result<TrieNode> {
        // Try cache first
        if let Some(node) = self.state_cache.nodes.get(hash) {
            self.metrics.hits.get(&CacheType::StateNode).unwrap().inc();
            return Ok(node.clone());
        }
        
        // Cache miss, get from database
        self.metrics.misses.get(&CacheType::StateNode).unwrap().inc();
        let node = db_get()?;
        
        // Update cache
        self.state_cache.nodes.put(*hash, node.clone());
        self.update_cache_metrics();
        
        Ok(node)
    }
    
    fn prefetch_blocks(&mut self, hashes: &[BlockHash], db_batch_get: impl FnOnce(&[BlockHash]) -> Result<Vec<Block>>) -> Result<()> {
        // Filter out blocks already in cache
        let missing_hashes: Vec<_> = hashes.iter()
            .filter(|h| !self.block_cache.contains_key(h))
            .cloned()
            .collect();
            
        if missing_hashes.is_empty() {
            return Ok(());
        }
        
        // Fetch missing blocks in a batch
        let blocks = db_batch_get(&missing_hashes)?;
        
        // Add to cache
        for block in blocks {
            let hash = block.hash();
            self.block_cache.put(hash, block);
        }
        
        self.update_cache_metrics();
        
        Ok(())
    }
    
    fn update_cache_metrics(&self) {
        // Update size metrics
        self.metrics.sizes.get(&CacheType::Block).unwrap()
            .set(self.block_cache.len() as f64);
            
        self.metrics.sizes.get(&CacheType::Header).unwrap()
            .set(self.header_cache.len() as f64);
            
        self.metrics.sizes.get(&CacheType::StateNode).unwrap()
            .set(self.state_cache.nodes.len() as f64);
            
        self.metrics.sizes.get(&CacheType::Account).unwrap()
            .set(self.state_cache.account_values.len() as f64);
            
        self.metrics.sizes.get(&CacheType::Storage).unwrap()
            .set(self.state_cache.storage_values.len() as f64);
            
        self.metrics.sizes.get(&CacheType::Transaction).unwrap()
            .set(self.transaction_cache.len() as f64);
            
        self.metrics.sizes.get(&CacheType::Receipt).unwrap()
            .set(self.receipt_cache.len() as f64);
            
        // Estimate memory usage (simplified)
        let total_entries = 
            self.block_cache.len() * BLOCK_AVERAGE_SIZE +
            self.header_cache.len() * HEADER_SIZE +
            self.state_cache.nodes.len() * NODE_AVERAGE_SIZE +
            self.state_cache.account_values.len() * ACCOUNT_SIZE +
            self.state_cache.storage_values.len() * STORAGE_VALUE_SIZE +
            self.transaction_cache.len() * TRANSACTION_AVERAGE_SIZE +
            self.receipt_cache.len() * RECEIPT_AVERAGE_SIZE;
            
        self.metrics.memory_usage.set(total_entries as f64);
    }
    
    fn optimize_cache_sizes(&mut self, usage_stats: &CacheUsageStats) -> CacheOptimizations {
        // Dynamically adjust cache sizes based on hit rates and memory pressure
        let mut optimizations = CacheOptimizations::default();
        
        // If we're under memory pressure, shrink the least effective caches
        if usage_stats.memory_pressure > 0.9 {
            // Find cache with lowest hit rate
            let mut lowest_hit_rate = 1.0;
            let mut lowest_type = None;
            
            for (cache_type, stats) in &usage_stats.hit_rates {
                if stats.hit_rate < lowest_hit_rate {
                    lowest_hit_rate = stats.hit_rate;
                    lowest_type = Some(cache_type);
                }
            }
            
            // Reduce size of cache with lowest hit rate
            if let Some(cache_type) = lowest_type {
                match cache_type {
                    CacheType::Block => {
                        let new_size = (self.block_cache.capacity() as f64 * 0.8) as usize;
                        optimizations.resize_block_cache = Some(new_size);
                    },
                    // Similar cases for other cache types
                    // ...existing code...
                }
            }
        }
        
        // If a cache has very high hit rate, consider increasing its size
        for (cache_type, stats) in &usage_stats.hit_rates {
            if stats.hit_rate > 0.95 && stats.eviction_rate > 0.1 {
                // This cache is effective but evicting too much
                match cache_type {
                    CacheType::Block => {
                        let new_size = (self.block_cache.capacity() as f64 * 1.2) as usize;
                        optimizations.resize_block_cache = Some(new_size);
                    },
                    // Similar cases for other cache types
                    // ...existing code...
                }
            }
        }
        
        optimizations
    }
}
```

**Design Rationale**:
- **Tiered Caching**: Multiple specialized caches for different data types
- **Memory Efficiency**: LRU eviction to prioritize keeping recently used items
- **Performance Monitoring**: Tracks hits, misses, and evictions for optimization
- **Auto-tuning**: Can adjust cache sizes based on usage patterns

**For Beginners**: Caching strategies are like keeping your most frequently used tools on your workbench instead of having to go to the toolbox each time. The system remembers recently used data in memory, making access much faster than reading from disk repeatedly.

## 8.2 Batch Processing
Optimizing database operations by grouping them together.

```rust
struct BatchProcessor {
    db: Database,
    max_batch_size: usize,
    pending_operations: WriteBatch,
    operation_count: usize,
    auto_commit_threshold: usize,
    metrics: BatchMetrics,
}

struct BatchMetrics {
    batch_sizes: Histogram,
    batch_operation_types: CounterVec,
    batch_durations: Histogram,
    operations_per_second: Gauge,
    avg_operation_size: Gauge,
}

impl BatchProcessor {
    fn add_operation(&mut self, op: WriteOperation) -> Result<()> {
        // Add operation to batch
        match op {
            WriteOperation::Put { key, value } => {
                self.pending_operations.put(key, value);
                self.metrics.batch_operation_types.with_label_values(&["put"]).inc();
            },
            WriteOperation::Delete { key } => {
                self.pending_operations.delete(key);
                self.metrics.batch_operation_types.with_label_values(&["delete"]).inc();
            },
            WriteOperation::SingleDelete { key } => {
                self.pending_operations.single_delete(key);
                self.metrics.batch_operation_types.with_label_values(&["single_delete"]).inc();
            },
            WriteOperation::DeleteRange { start_key, end_key } => {
                self.pending_operations.delete_range(start_key, end_key);
                self.metrics.batch_operation_types.with_label_values(&["delete_range"]).inc();
            },
            WriteOperation::Merge { key, value } => {
                self.pending_operations.merge(key, value);
                self.metrics.batch_operation_types.with_label_values(&["merge"]).inc();
            },
        }
        
        self.operation_count += 1;
        
        // Auto-commit if threshold reached
        if self.operation_count >= self.auto_commit_threshold {
            self.commit_batch()?;
        }
        
        Ok(())
    }
    
    fn commit_batch(&mut self) -> Result<usize> {
        if self.operation_count == 0 {
            return Ok(0);
        }
        
        let operations_count = self.operation_count;
        let start_time = Instant::now();
        
        // Perform the write operation
        self.db.write(&self.pending_operations)?;
        
        // Update metrics
        let duration = start_time.elapsed();
        self.metrics.batch_sizes.observe(operations_count as f64);
        self.metrics.batch_durations.observe(duration.as_secs_f64());
        self.metrics.operations_per_second.set(
            operations_count as f64 / duration.as_secs_f64()
        );
        
        // Reset for next batch
        self.pending_operations = WriteBatch::new();
        self.operation_count = 0;
        
        Ok(operations_count)
    }
    
    fn optimize_batch_size(&mut self) -> Result<()> {
        // Analyze metrics to determine optimal batch size
        let avg_batch_duration = self.metrics.batch_durations.get_sample_sum() / 
                                self.metrics.batch_durations.get_sample_count();
                                
        let avg_batch_size = self.metrics.batch_sizes.get_sample_sum() / 
                            self.metrics.batch_sizes.get_sample_count();
                            
        let operations_per_second = avg_batch_size / avg_batch_duration;
        
        // If operations per second is below threshold, increase batch size
        if operations_per_second < TARGET_OPS_PER_SECOND && avg_batch_duration < MAX_BATCH_DURATION {
            self.auto_commit_threshold = (self.auto_commit_threshold as f64 * 1.2).min(MAX_BATCH_SIZE as f64) as usize;
        }
        // If batch duration is too high, decrease batch size
        else if avg_batch_duration > MAX_BATCH_DURATION {
            self.auto_commit_threshold = (self.auto_commit_threshold as f64 * 0.8).max(MIN_BATCH_SIZE as f64) as usize;
        }
        
        log::debug!("Optimized batch size to {}", self.auto_commit_threshold);
        
        Ok(())
    }
    
    fn process_operations_batch(&mut self, operations: Vec<WriteOperation>) -> Result<()> {
        let start = Instant::now();
        
        // Group operations to optimize for database access patterns
        let mut ordered_operations = Vec::with_capacity(operations.len());
        ordered_operations.extend(operations);
        
        // Sort operations to improve database performance
        // (e.g., group by key prefix to improve locality)
        ordered_operations.sort_by(|a, b| {
            a.get_key_prefix().cmp(&b.get_key_prefix())
        });
        
        // Process sorted operations
        for op in ordered_operations {
            self.add_operation(op)?;
        }
        
        // Ensure any remaining operations are committed
        self.commit_batch()?;
        
        log::debug!("Processed batch of {} operations in {:?}", 
                   operations.len(), start.elapsed());
        
        Ok(())
    }
}
```

**Design Rationale**:
- **Reduced I/O**: Minimizes disk operations by grouping multiple changes
- **Atomicity**: All-or-nothing writes maintain database consistency
- **Adaptive Sizing**: Self-tuning batch sizes based on performance metrics
- **Operation Ordering**: Groups related operations to improve locality

**For Beginners**: Batch processing is like going to the grocery store with a complete shopping list instead of making separate trips for each item. By combining many database operations into a single "trip," we significantly reduce the overhead and make the whole system more efficient.

## 8.3 Database Tuning
Optimizing the underlying database engine for blockchain-specific workloads.

```rust
struct DatabaseTuner {
    db: Database,
    config: DatabaseConfig,
    metrics: DatabaseMetrics,
    tuning_interval: Duration,
    last_tuning: Instant,
}

struct DatabaseConfig {
    write_buffer_size: usize,
    max_write_buffer_number: i32,
    target_file_size_base: usize,
    level0_file_num_compaction_trigger: i32,
    max_background_jobs: i32,
    compression_type: CompressionType,
    block_cache_size: usize,
    bloom_filter_bits_per_key: i32,
}

enum CompressionType {
    None,
    Snappy,
    Zlib,
    Lz4,
    Zstd,
}

impl DatabaseTuner {
    fn tune_database_config(&mut self) -> Result<()> {
        let now = Instant::now();
        
        // Only tune at specified intervals
        if now.duration_since(self.last_tuning) < self.tuning_interval {
            return Ok(());
        }
        
        self.last_tuning = now;
        log::info!("Performing database tuning");
        
        // Collect current metrics
        let metrics = self.collect_database_metrics()?;
        
        // Analyze and adjust write buffer configuration
        self.tune_write_buffer(&metrics)?;
        
        // Adjust compaction settings
        self.tune_compaction_settings(&metrics)?;
        
        // Adjust cache size
        self.tune_cache_size(&metrics)?;
        
        // Adjust compression
        self.tune_compression(&metrics)?;
        
        log::info!("Database tuning complete. New config: {:?}", self.config);
        
        Ok(())
    }
    
    fn tune_write_buffer(&mut self, metrics: &DatabaseMetricsSnapshot) -> Result<()> {
        // If write stalls are occurring, increase write buffer
        if metrics.write_stall_duration > Duration::from_secs(5) {
            let new_size = (self.config.write_buffer_size as f64 * 1.5) as usize;
            self.update_write_buffer_size(new_size)?;
            
            log::info!("Increased write buffer size to {} due to write stalls", new_size);
        }
        
        // If memory usage is too high, decrease write buffer
        if metrics.memory_usage_ratio > 0.9 {
            let new_size = (self.config.write_buffer_size as f64 * 0.8) as usize;
            self.update_write_buffer_size(new_size)?;
            
            log::info!("Decreased write buffer size to {} due to high memory usage", new_size);
        }
        
        Ok(())
    }
    
    fn tune_compaction_settings(&mut self, metrics: &DatabaseMetricsSnapshot) -> Result<()> {
        // If compaction is falling behind (too many L0 files), adjust compaction triggers
        if metrics.l0_file_count > self.config.level0_file_num_compaction_trigger as u64 * 2 {
            // More aggressive compaction trigger
            let new_trigger = (self.config.level0_file_num_compaction_trigger as f64 * 0.75) as i32;
            self.update_compaction_trigger(new_trigger)?;
            
            // Increase background compaction threads
            let new_bg_jobs = (self.config.max_background_jobs + 2).min(MAX_BACKGROUND_JOBS);
            self.update_background_jobs(new_bg_jobs)?;
            
            log::info!("Adjusted compaction settings: trigger={}, bg_jobs={}", 
                     new_trigger, new_bg_jobs);
        }
        
        // If disk space is critical, try to increase compaction speed
        if metrics.disk_space_remaining_ratio < 0.1 {
            // More aggressive compaction and smaller target file size to reclaim space
            let new_file_size = (self.config.target_file_size_base as f64 * 0.5) as usize;
            self.update_target_file_size(new_file_size)?;
            
            log::warn!("Disk space critical ({}% remaining). Adjusted target file size to {}",
                     (metrics.disk_space_remaining_ratio * 100.0).round(), new_file_size);
        }
        
        Ok(())
    }
    
    fn tune_cache_size(&mut self, metrics: &DatabaseMetricsSnapshot) -> Result<()> {
        // If cache hit rate is low but memory available, increase cache
        if metrics.block_cache_hit_ratio < 0.8 && metrics.memory_usage_ratio < 0.7 {
            let new_size = (self.config.block_cache_size as f64 * 1.2) as usize;
            self.update_block_cache_size(new_size)?;
            
            log::info!("Increased block cache size to {} due to low hit ratio", new_size);
        }
        
        // If cache hit rate is high and cache is small, might benefit from more cache
        if metrics.block_cache_hit_ratio > 0.95 && 
           metrics.memory_usage_ratio < 0.5 && 
           self.config.block_cache_size < MAX_RECOMMENDED_CACHE_SIZE {
            let new_size = (self.config.block_cache_size as f64 * 1.5) as usize;
            self.update_block_cache_size(new_size)?;
            
            log::info!("Increased block cache size to {} to improve performance", new_size);
        }
        
        Ok(())
    }
    
    fn tune_compression(&mut self, metrics: &DatabaseMetricsSnapshot) -> Result<()> {
        // If CPU usage is high but disk space plentiful, reduce compression
        if metrics.cpu_usage_ratio > 0.9 && metrics.disk_space_remaining_ratio > 0.3 {
            // Choose less CPU-intensive compression
            let new_compression = match self.config.compression_type {
                CompressionType::Zstd => CompressionType::Lz4,
                CompressionType::Zlib => CompressionType::Snappy,
                _ => self.config.compression_type.clone(), // No change for already light compression
            };
            
            if new_compression != self.config.compression_type {
                self.update_compression_type(new_compression)?;
                
                log::info!("Changed compression to {:?} to reduce CPU usage", new_compression);
            }
        }
        
        // If disk space is low, increase compression
        if metrics.disk_space_remaining_ratio < 0.15 {
            // Choose more space-efficient compression
            let new_compression = match self.config.compression_type {
                CompressionType::None => CompressionType::Snappy,
                CompressionType::Snappy => CompressionType::Lz4,
                CompressionType::Lz4 => CompressionType::Zstd,
                _ => self.config.compression_type.clone(), // No change if already using high compression
            };
            
            if new_compression != self.config.compression_type {
                self.update_compression_type(new_compression)?;
                
                log::info!("Changed compression to {:?} to save disk space", new_compression);
            }
        }
        
        Ok(())
    }
    
    fn update_write_buffer_size(&mut self, new_size: usize) -> Result<()> {
        // Apply change to database
        self.db.set_options(&[("write_buffer_size", &new_size.to_string())])?;
        
        // Update config
        self.config.write_buffer_size = new_size;
        
        Ok(())
    }
    
    // Additional update methods for other settings
    // ...existing code...
}
```

**Design Rationale**:
- **Workload Awareness**: Tunes parameters based on specific blockchain access patterns
- **Resource Balancing**: Optimizes tradeoffs between CPU, memory, and disk usage
- **Adaptive Configuration**: Responds to changing workloads and resource availability
- **Safe Adjustments**: Makes incremental changes with validation

**For Beginners**: Database tuning is like adjusting the gears on a bicycle for different terrain. As the blockchain grows or usage patterns change, the storage system automatically adjusts its internal settings to maintain optimal performance, whether you're climbing a steep hill (high write load) or cruising on flat ground (mostly reads).

[Back to Index](./03-0-storage-layer-index.md) | [Previous: Data Integrity](./03.07-storage-layer-data-integrity.md) | [Next: References](./03.09-storage-layer-references.md)
