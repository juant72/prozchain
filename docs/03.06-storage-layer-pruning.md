# 6. Pruning Mechanisms

## 6.1 Pruning Strategies
Manages blockchain data growth by selectively removing historical data.

```rust
enum PruningMode {
    Archive,                // No pruning, keep all history
    Full,                   // Aggressive pruning, minimal state
    Custom {
        blocks_to_keep: u64,
        state_history_blocks: u64,
    },
}

struct PruningManager {
    mode: PruningMode,
    db: Database,
    chain_head: BlockHeight,
    last_pruned_height: BlockHeight,
    pruning_interval: BlockHeight,
}

impl PruningManager {
    fn new(mode: PruningMode, db: Database, chain_head: BlockHeight) -> Self {
        PruningManager {
            mode,
            db,
            chain_head,
            last_pruned_height: 0,
            pruning_interval: 1000, // Run pruning every 1000 blocks
        }
    }
    
    fn run_pruning_cycle(&mut self, new_head: BlockHeight) -> Result<PruningStats> {
        // Update chain head
        self.chain_head = new_head;
        
        // Check if pruning is disabled
        if let PruningMode::Archive = self.mode {
            return Ok(PruningStats::default());
        }
        
        // Check if it's time to run pruning
        if self.chain_head < self.last_pruned_height + self.pruning_interval {
            return Ok(PruningStats::default());
        }
        
        // Determine pruning threshold based on mode
        let pruning_threshold = match &self.mode {
            PruningMode::Archive => {
                return Ok(PruningStats::default());  // No pruning
            },
            PruningMode::Full => {
                // Keep minimal recent history
                self.chain_head.saturating_sub(MIN_BLOCKS_TO_KEEP)
            },
            PruningMode::Custom { blocks_to_keep, .. } => {
                self.chain_head.saturating_sub(*blocks_to_keep)
            }
        };
        
        // Nothing to prune yet
        if pruning_threshold <= self.last_pruned_height || pruning_threshold == 0 {
            return Ok(PruningStats::default());
        }
        
        // Perform pruning for each data type
        let mut stats = PruningStats::default();
        
        stats.state_nodes_removed = self.prune_state_history(pruning_threshold)?;
        stats.receipts_removed = self.prune_receipts(pruning_threshold)?;
        stats.logs_removed = self.prune_logs(pruning_threshold)?;
        
        // Update last pruned height
        self.last_pruned_height = pruning_threshold;
        
        // Store pruning metadata
        self.store_pruning_metadata()?;
        
        Ok(stats)
    }
    
    fn prune_state_history(&mut self, threshold: BlockHeight) -> Result<usize> {
        // Determine state history threshold based on pruning mode
        let state_threshold = match &self.mode {
            PruningMode::Archive => {
                return Ok(0);  // No pruning for archive nodes
            },
            PruningMode::Full => {
                // Keep minimal state history
                threshold
            },
            PruningMode::Custom { state_history_blocks, .. } => {
                // Use custom threshold
                self.chain_head.saturating_sub(*state_history_blocks)
            }
        };
        
        // Find state roots to prune
        let mut roots_to_prune = Vec::new();
        let mut state_nodes_to_prune = Vec::new();
        
        // Find state roots older than the threshold that aren't snapshot roots
        let iter = self.db.iter_prefix(&make_key_prefix(KeyPrefix::StateRootAtHeight))?;
        for item in iter {
            let (key, value) = item?;
            
            // Extract height from key
            let height = extract_height_from_key(&key)?;
            
            if height < state_threshold {
                let root = StateRoot::from_slice(&value)?;
                
                // Don't prune roots used by snapshots
                if !self.is_snapshot_root(&root)? {
                    roots_to_prune.push((height, root));
                    
                    // Find state nodes for this root
                    let nodes = self.find_state_nodes_for_root(&root)?;
                    state_nodes_to_prune.extend(nodes);
                }
            }
        }
        
        // Perform actual deletion in batches
        let batch_size = 1000;
        let mut batch = WriteBatch::new();
        let mut removed_count = 0;
        
        for node_key in state_nodes_to_prune {
            batch.delete(node_key);
            removed_count += 1;
            
            if removed_count % batch_size == 0 {
                self.db.write(batch.clone())?;
                batch = WriteBatch::new();
            }
        }
        
        // Delete remaining nodes
        if !batch.is_empty() {
            self.db.write(batch)?;
        }
        
        // Delete state root references
        let mut batch = WriteBatch::new();
        for (height, _) in roots_to_prune {
            let key = make_key(KeyPrefix::StateRootAtHeight, &height);
            batch.delete(key);
        }
        self.db.write(batch)?;
        
        Ok(removed_count)
    }
    
    fn is_snapshot_root(&self, root: &StateRoot) -> Result<bool> {
        // Check if this root is used by a snapshot
        let prefix = make_key_prefix(KeyPrefix::SnapshotMetadata);
        let iter = self.db.iter_prefix(&prefix)?;
        
        for item in iter {
            let (_, value) = item?;
            let metadata: SnapshotMetadata = deserialize(&value)?;
            
            if &metadata.state_root == root {
                return Ok(true);
            }
        }
        
        Ok(false)
    }
}
```

**Design Rationale**:
- **Mode Flexibility**: Different pruning strategies for different node types
- **Batched Deletion**: Efficient removal of large data sets
- **Snapshot Protection**: Preserves data needed for snapshots
- **Controlled Growth**: Balances disk usage with data availability needs

**For Beginners**: Pruning is like cleaning out old files from your computer to save space. Instead of keeping every version of every account state since the beginning of time, nodes can throw away very old data that's rarely needed, while keeping recent history and important checkpoints.

## 6.2 Archive Nodes
Specialized nodes that maintain complete blockchain history.

```rust
struct ArchiveNodeConfig {
    cold_storage_path: Option<PathBuf>,
    compression_level: CompressionLevel,
    index_level: ArchiveIndexLevel,
}

struct ArchiveNode {
    config: ArchiveNodeConfig,
    hot_storage: Database,
    cold_storage: Option<ColdStorageManager>,
    indexer: ArchiveIndexer,
}

enum CompressionLevel {
    None,
    Fast,
    Default,
    Maximum,
}

enum ArchiveIndexLevel {
    Basic,      // Block headers and transaction hashes
    Standard,   // Basic + account history
    Complete,   // All data deeply indexed
}

impl ArchiveNode {
    fn new(config: ArchiveNodeConfig, hot_storage: Database) -> Result<Self> {
        let cold_storage = if let Some(path) = &config.cold_storage_path {
            Some(ColdStorageManager::new(path, &config)?)
        } else {
            None
        };
        
        let indexer = ArchiveIndexer::new(&config.index_level, &hot_storage)?;
        
        Ok(ArchiveNode {
            config,
            hot_storage,
            cold_storage,
            indexer,
        })
    }
    
    fn process_block(&mut self, block: &Block, receipts: &[Receipt]) -> Result<()> {
        // Archive all block data
        self.store_complete_block(block, receipts)?;
        
        // Update indices for efficient querying
        self.indexer.index_block(block, receipts)?;
        
        // Check if older blocks should be moved to cold storage
        if let Some(cold_storage) = &mut self.cold_storage {
            cold_storage.migrate_old_blocks(self.indexer.get_latest_height())?;
        }
        
        Ok(())
    }
    
    fn store_complete_block(&mut self, block: &Block, receipts: &[Receipt]) -> Result<()> {
        // Store full block data without pruning
        let mut batch = WriteBatch::new();
        
        // Store block
        let block_key = make_key(KeyPrefix::BlockByHash, &block.hash());
        batch.put(&block_key, &serialize_with_compression(block, &self.config.compression_level)?);
        
        // Store header
        let header_key = make_key(KeyPrefix::HeaderByHash, &block.hash());
        batch.put(&header_key, &serialize(&block.header)?);
        
        // Store receipts
        for (i, receipt) in receipts.iter().enumerate() {
            let tx_hash = &block.transactions[i].hash();
            let receipt_key = make_key(KeyPrefix::Receipt, tx_hash);
            batch.put(&receipt_key, &serialize_with_compression(receipt, &self.config.compression_level)?);
        }
        
        // Store height to hash mapping
        let height_key = make_key(KeyPrefix::HashByHeight, &block.header.height);
        batch.put(&height_key, &block.hash().as_bytes());
        
        // Commit batch
        self.hot_storage.write(batch)?;
        
        Ok(())
    }
    
    fn query_historical_state(&self, block_height: BlockHeight, address: &Address) -> Result<Option<Account>> {
        // Find state root at this height
        let state_root = self.get_state_root_at_height(block_height)?;
        
        // Query account state at this root
        self.indexer.get_account_at_root(&state_root, address)
    }
}

struct ColdStorageManager {
    storage_path: PathBuf,
    migrate_threshold: BlockHeight,
    compression: CompressionLevel,
}

impl ColdStorageManager {
    fn migrate_old_blocks(&mut self, current_height: BlockHeight) -> Result<()> {
        let migration_height = current_height.saturating_sub(self.migrate_threshold);
        if migration_height <= 0 {
            return Ok(());
        }
        
        // Find blocks to migrate
        let blocks_to_migrate = self.find_blocks_to_migrate(migration_height)?;
        
        // Migrate each block
        for block_hash in blocks_to_migrate {
            self.migrate_block_to_cold_storage(&block_hash)?;
        }
        
        Ok(())
    }
    
    fn migrate_block_to_cold_storage(&mut self, block_hash: &BlockHash) -> Result<()> {
        // Load block data from hot storage
        let block = self.load_block_from_hot_storage(block_hash)?;
        
        // Compress and store to cold storage
        let compressed = serialize_with_compression(&block, &self.compression)?;
        let cold_path = self.get_cold_storage_path(block_hash);
        
        // Ensure directory exists
        if let Some(parent) = cold_path.parent() {
            fs::create_dir_all(parent)?;
        }
        
        // Write to cold storage
        fs::write(&cold_path, &compressed)?;
        
        // Update index to indicate block is in cold storage
        self.update_cold_storage_index(block_hash, &block.header.height)?;
        
        Ok(())
    }
    
    fn get_cold_storage_path(&self, block_hash: &BlockHash) -> PathBuf {
        // Create hierarchical path based on hash to avoid too many files in one directory
        let hash_hex = hex::encode(block_hash);
        let dir1 = &hash_hex[0..2];
        let dir2 = &hash_hex[2..4];
        
        self.storage_path.join(dir1).join(dir2).join(format!("{}.block", hash_hex))
    }
}
```

**Design Rationale**:
- **Complete History**: Maintains all historical blockchain data
- **Tiered Storage**: Hot storage for recent data, cold storage for older data
- **Deep Indexing**: Advanced querying of historical states
- **Compression**: Reduces storage requirements while maintaining all data

**For Beginners**: Archive nodes are like library archives that keep every book ever published, while regular nodes only keep recent and popular books. They require more storage space but allow you to research the complete history of the blockchain.

## 6.3 Data Recovery
Mechanisms for recovering from data corruption or inconsistency.

```rust
struct RecoveryManager {
    storage: Arc<StorageEngine>,
    network: Arc<NetworkService>,
    recovery_config: RecoveryConfig,
}

struct RecoveryConfig {
    max_block_request_batch: usize,
    verification_depth: u32,
    timeout_per_batch: Duration,
    max_peers_to_query: usize,
}

impl RecoveryManager {
    async fn recover_missing_block(&mut self, height: BlockHeight) -> Result<Block> {
        log::info!("Recovering missing block at height {}", height);
        
        // Get list of peers to query
        let peers = self.network.get_synchronized_peers()?;
        
        if peers.is_empty() {
            return Err(Error::NoPeersForRecovery);
        }
        
        // Try to recover from peers
        for peer in peers.iter().take(self.recovery_config.max_peers_to_query) {
            match self.request_block_from_peer(peer, height).await {
                Ok(block) => {
                    // Verify block integrity
                    if self.verify_recovered_block(&block).is_ok() {
                        // Store block
                        self.storage.block_store.store_block(&block)?;
                        return Ok(block);
                    }
                },
                Err(e) => {
                    log::debug!("Failed to recover block {} from {}: {}", height, peer, e);
                }
            }
        }
        
        Err(Error::BlockRecoveryFailed(height))
    }
    
    async fn recover_state_range(&mut self, from: BlockHeight, to: BlockHeight) -> Result<()> {
        log::info!("Recovering state for block range {} to {}", from, to);
        
        // Find closest available snapshot before 'from'
        let (snapshot_height, snapshot_root) = self.find_closest_snapshot_before(from)?;
        
        // Get list of blocks we need to apply
        let blocks_to_apply = self.get_blocks_for_recovery(snapshot_height, to)?;
        
        // Start from snapshot state
        let mut current_state = StateReconstructor::new(self.storage.clone(), snapshot_root);
        
        // Apply each block's transactions to reconstruct state
        for block in blocks_to_apply {
            current_state.apply_block(&block)?;
            
            // Store state root for this height
            self.storage.state_database.store_state_root_at_height(
                block.header.height,
                current_state.state_root()
            )?;
            
            log::debug!("Recovered state at height {}", block.header.height);
        }
        
        log::info!("State recovery completed for range {} to {}", from, to);
        
        Ok(())
    }
    
    fn find_closest_snapshot_before(&self, height: BlockHeight) -> Result<(BlockHeight, StateRoot)> {
        let mut snapshot_height = 0;
        let mut snapshot_root = StateRoot::default();
        
        // Find all snapshots
        let prefix = make_key_prefix(KeyPrefix::SnapshotMetadata);
        let iter = self.storage.db.iter_prefix(&prefix)?;
        
        for item in iter {
            let (_, value) = item?;
            let metadata: SnapshotMetadata = deserialize(&value)?;
            
            if metadata.height < height && metadata.height > snapshot_height {
                snapshot_height = metadata.height;
                snapshot_root = metadata.state_root;
            }
        }
        
        if snapshot_height == 0 {
            // No snapshot found, use genesis
            return self.get_genesis_state();
        }
        
        Ok((snapshot_height, snapshot_root))
    }
    
    async fn repair_database_inconsistencies(&mut self) -> Result<RepairStats> {
        let mut stats = RepairStats::default();
        
        // Check block height continuity
        stats.missing_blocks = self.repair_block_height_continuity().await?;
        
        // Check state root consistency
        stats.state_root_fixes = self.repair_state_root_consistency().await?;
        
        // Check index integrity
        stats.index_repairs = self.repair_indices().await?;
        
        log::info!("Database repair completed: {:?}", stats);
        
        Ok(stats)
    }
}

struct StateReconstructor {
    storage: Arc<StorageEngine>,
    current_root: StateRoot,
    trie_cache: TrieCache,
}

impl StateReconstructor {
    fn apply_block(&mut self, block: &Block) -> Result<()> {
        // Verify that block's parent state root matches our current root
        if block.header.parent_state_root != self.current_root {
            return Err(Error::StateMismatch);
        }
        
        // Get transactions from block
        for tx in &block.transactions {
            // Apply transaction to current state
            self.apply_transaction(tx)?;
        }
        
        // Verify that our computed state root matches the block's state root
        if self.current_root != block.header.state_root {
            return Err(Error::StateRootMismatch);
        }
        
        Ok(())
    }
}
```

**Design Rationale**:
- **Peer-Based Recovery**: Fetches missing data from network peers
- **Verification**: Ensures recovered data is valid and consistent
- **State Reconstruction**: Rebuilds state from transaction history
- **Automated Repair**: Fixes common inconsistencies without manual intervention

**For Beginners**: Data recovery mechanisms are like having backup systems and recovery tools for your data. If some blockchain data gets corrupted or lost, these tools can fetch missing data from other nodes in the network and verify it's correct before using it.

[Back to Index](./03-0-storage-layer-index.md) | [Previous: Transaction Storage](./03.05-storage-layer-transaction-storage.md) | [Next: Data Integrity](./03.07-storage-layer-data-integrity.md)
