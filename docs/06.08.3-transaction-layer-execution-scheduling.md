# 8.3 Execution Scheduling

Execution scheduling manages how transactions are allocated to processing resources while optimizing for throughput, latency, and fairness.

## Scheduler Architecture

```rust
struct ExecutionScheduler {
    /// Worker pool for transaction execution
    worker_pool: Arc<WorkerPool>,
    
    /// Prioritization engine
    prioritizer: Arc<TransactionPrioritizer>,
    
    /// Load balancer for distributing work
    load_balancer: Arc<LoadBalancer>,
    
    /// Scheduling policy
    scheduling_policy: SchedulingPolicy,
    
    /// Metrics collection
    metrics: SchedulerMetrics,
}

struct SchedulerMetrics {
    scheduled_transactions: AtomicU64,
    total_execution_time_ns: AtomicU64,
    avg_wait_time_ns: AtomicU64,
    avg_execution_time_ns: AtomicU64,
    worker_utilization: AtomicU64,
}

enum SchedulingPolicy {
    /// First come, first served
    FCFS,
    
    /// Higher gas price first
    HighestGasPrice,
    
    /// Shortest job first (estimated execution time)
    ShortestJobFirst,
    
    /// Round-robin across workers
    RoundRobin,
    
    /// Work-stealing between workers
    WorkStealing,
    
    /// Multi-level feedback queue
    MultilevelFeedback {
        queue_count: usize,
        time_slice_ns: u64,
    },
}
```

## Job Creation and Management

```rust
/// Represents a single unit of work to be executed
struct ExecutionJob {
    /// Unique job identifier
    id: JobId,
    
    /// Transaction to execute
    transaction: Transaction,
    
    /// Time when job was created
    creation_time: Instant,
    
    /// Priority value (higher = more important)
    priority: f64,
    
    /// Estimated execution time in nanoseconds
    estimated_duration_ns: u64,
    
    /// Dependencies that must complete before this job
    dependencies: HashSet<JobId>,
    
    /// Current job status
    status: JobStatus,
}

enum JobStatus {
    Created,
    Ready,
    Running,
    Completed(TransactionResult),
    Failed(Error),
}

struct JobQueue {
    /// Jobs organized by priority
    priority_queue: BinaryHeap<JobWithPriority>,
    
    /// Jobs indexed by ID for quick lookup
    jobs_by_id: HashMap<JobId, Arc<Mutex<ExecutionJob>>>,
    
    /// Jobs organized by dependency
    dependency_graph: DependencyGraph<JobId>,
    
    /// Size limit for the queue
    capacity: usize,
}

impl JobQueue {
    /// Add a job to the queue
    fn push(&mut self, job: ExecutionJob) -> Result<(), Error> {
        // Check capacity
        if self.jobs_by_id.len() >= self.capacity {
            return Err(Error::QueueFull);
        }
        
        let job_id = job.id;
        
        // Wrap job in thread-safe container
        let job_arc = Arc::new(Mutex::new(job));
        
        // Add to priority queue
        let priority = {
            let job = job_arc.lock().unwrap();
            JobWithPriority {
                id: job_id,
                priority: job.priority,
            }
        };
        
        self.priority_queue.push(priority);
        
        // Add to lookup map
        self.jobs_by_id.insert(job_id, job_arc.clone());
        
        // Add to dependency graph
        self.dependency_graph.add_node(job_id);
        
        // Add dependency edges
        {
            let job = job_arc.lock().unwrap();
            for dep_id in &job.dependencies {
                self.dependency_graph.add_edge(*dep_id, job_id);
            }
        }
        
        Ok(())
    }
    
    /// Get the next job that's ready for execution
    fn pop_ready(&mut self) -> Option<Arc<Mutex<ExecutionJob>>> {
        // Get jobs with no dependencies
        let ready_jobs: Vec<_> = self.dependency_graph.get_ready_nodes().collect();
        
        if ready_jobs.is_empty() {
            return None;
        }
        
        // Find the highest priority job among ready jobs
        let mut best_job = None;
        let mut best_priority = f64::NEG_INFINITY;
        
        for job_id in ready_jobs {
            if let Some(job) = self.jobs_by_id.get(&job_id) {
                let priority = job.lock().unwrap().priority;
                if priority > best_priority {
                    best_priority = priority;
                    best_job = Some(job_id);
                }
            }
        }
        
        // Remove the job from our collections
        if let Some(job_id) = best_job {
            self.dependency_graph.remove_node(job_id);
            
            // Remove from priority queue by marking as processed
            // (actual removal happens lazily during next push/pop)
            self.priority_queue.remove_by_id(job_id);
            
            // Return the job
            return self.jobs_by_id.remove(&job_id);
        }
        
        None
    }
    
    /// Mark a job as completed
    fn complete_job(&mut self, job_id: JobId, result: TransactionResult) {
        // Update job status
        if let Some(job) = self.jobs_by_id.get(&job_id) {
            let mut job = job.lock().unwrap();
            job.status = JobStatus::Completed(result);
        }
        
        // Update dependency graph to release dependent jobs
        self.dependency_graph.mark_completed(job_id);
    }
}
```

## Worker Pool Management

```rust
struct WorkerPool {
    /// Available worker threads
    workers: Vec<Worker>,
    
    /// Job queue
    job_queue: Arc<JobQueue>,
    
    /// Pool configuration
    config: WorkerPoolConfig,
    
    /// Worker status
    worker_status: Arc<RwLock<Vec<WorkerStatus>>>,
}

struct Worker {
    /// Worker ID
    id: WorkerId,
    
    /// Worker thread handle
    thread: Option<JoinHandle<()>>,
    
    /// Channel to communicate with worker
    sender: Sender<WorkerCommand>,
    
    /// Channel to receive responses
    receiver: Receiver<WorkerResponse>,
}

enum WorkerStatus {
    Idle,
    Busy {
        current_job: JobId,
        started_at: Instant,
    },
    Unavailable,
}

enum WorkerCommand {
    ExecuteJob(Arc<Mutex<ExecutionJob>>),
    Pause,
    Resume,
    Shutdown,
}

enum WorkerResponse {
    JobCompleted {
        job_id: JobId,
        result: TransactionResult,
        execution_time: Duration,
    },
    JobFailed {
        job_id: JobId,
        error: Error,
        execution_time: Duration,
    },
    Ready,
    Paused,
    ShuttingDown,
}

impl WorkerPool {
    fn new(size: usize, config: WorkerPoolConfig) -> Result<Self, Error> {
        let job_queue = Arc::new(JobQueue::new(config.queue_capacity));
        let mut workers = Vec::with_capacity(size);
        let worker_status = Arc::new(RwLock::new(vec![WorkerStatus::Idle; size]));
        
        // Create and start worker threads
        for i in 0..size {
            let (worker_sender, worker_receiver) = mpsc::channel();
            let (response_sender, response_receiver) = mpsc::channel();
            
            let job_queue_clone = job_queue.clone();
            let worker_status_clone = worker_status.clone();
            
            let thread = thread::spawn(move || {
                Self::worker_loop(
                    WorkerId(i),
                    worker_receiver,
                    response_sender,
                    job_queue_clone,
                    worker_status_clone,
                );
            });
            
            workers.push(Worker {
                id: WorkerId(i),
                thread: Some(thread),
                sender: worker_sender,
                receiver: response_receiver,
            });
        }
        
        Ok(WorkerPool {
            workers,
            job_queue,
            config,
            worker_status,
        })
    }
    
    fn worker_loop(
        id: WorkerId,
        receiver: Receiver<WorkerCommand>,
        sender: Sender<WorkerResponse>,
        job_queue: Arc<JobQueue>,
        worker_status: Arc<RwLock<Vec<WorkerStatus>>>,
    ) {
        // Set worker as ready
        {
            let mut status = worker_status.write().unwrap();
            status[id.0] = WorkerStatus::Idle;
        }
        sender.send(WorkerResponse::Ready).unwrap();
        
        // Process commands
        while let Ok(cmd) = receiver.recv() {
            match cmd {
                WorkerCommand::ExecuteJob(job) => {
                    // Update status to busy
                    let job_id = job.lock().unwrap().id;
                    {
                        let mut status = worker_status.write().unwrap();
                        status[id.0] = WorkerStatus::Busy {
                            current_job: job_id,
                            started_at: Instant::now(),
                        };
                    }
                    
                    // Execute the job
                    let start_time = Instant::now();
                    let result = Self::execute_job(&job);
                    let execution_time = start_time.elapsed();
                    
                    // Send response
                    match result {
                        Ok(tx_result) => {
                            // Mark job as completed in queue
                            job_queue.complete_job(job_id, tx_result.clone());
                            
                            sender.send(WorkerResponse::JobCompleted {
                                job_id,
                                result: tx_result,
                                execution_time,
                            }).unwrap();
                        },
                        Err(error) => {
                            sender.send(WorkerResponse::JobFailed {
                                job_id,
                                error,
                                execution_time,
                            }).unwrap();
                        }
                    }
                    
                    // Update status back to idle
                    {
                        let mut status = worker_status.write().unwrap();
                        status[id.0] = WorkerStatus::Idle;
                    }
                },
                WorkerCommand::Pause => {
                    // Update status to unavailable
                    {
                        let mut status = worker_status.write().unwrap();
                        status[id.0] = WorkerStatus::Unavailable;
                    }
                    sender.send(WorkerResponse::Paused).unwrap();
                },
                WorkerCommand::Resume => {
                    // Update status to idle
                    {
                        let mut status = worker_status.write().unwrap();
                        status[id.0] = WorkerStatus::Idle;
                    }
                    sender.send(WorkerResponse::Ready).unwrap();
                },
                WorkerCommand::Shutdown => {
                    // Send shutdown response and exit loop
                    sender.send(WorkerResponse::ShuttingDown).unwrap();
                    break;
                }
            }
        }
    }
    
    fn execute_job(job: &Arc<Mutex<ExecutionJob>>) -> Result<TransactionResult, Error> {
        // In a real implementation, this would use the VM to execute the transaction
        // For this example, we'll just simulate execution with a delay
        
        let tx = {
            let mut job = job.lock().unwrap();
            job.status = JobStatus::Running;
            job.transaction.clone()
        };
        
        // Simulate execution time (proportional to gas limit)
        let sleep_duration = Duration::from_nanos(tx.gas_limit * 10);
        thread::sleep(sleep_duration);
        
        // Create a success result
        Ok(TransactionResult {
            status: 1,
            gas_used: tx.gas_limit / 2,
            return_data: Vec::new(),
            logs: Vec::new(),
        })
    }
}
```

## Load Balancing and Work Distribution

```rust
struct LoadBalancer {
    /// Statistics for job distribution
    stats: Arc<LoadBalancingStats>,
    
    /// Strategy for load balancing
    strategy: LoadBalancingStrategy,
    
    /// Access to worker pool
    worker_pool: Arc<WorkerPool>,
    
    /// Configuration
    config: LoadBalancerConfig,
}

struct LoadBalancingStats {
    /// Jobs assigned per worker
    jobs_per_worker: RwLock<Vec<u64>>,
    
    /// Current queue depth
    queue_depth: AtomicUsize,
    
    /// Execution time per worker
    execution_time_per_worker: RwLock<Vec<Duration>>,
    
    /// Recent worker utilization (0.0 - 1.0)
    worker_utilization: RwLock<Vec<f64>>,
}

enum LoadBalancingStrategy {
    /// Round-robin assignment
    RoundRobin {
        next_worker: AtomicUsize,
    },
    
    /// Least-loaded worker
    LeastLoaded,
    
    /// Fastest worker
    FastestWorker,
    
    /// Job-specific worker selection
    JobAffinity {
        affinity_function: Box<dyn Fn(&ExecutionJob) -> Option<WorkerId> + Send + Sync>,
    },
}

impl LoadBalancer {
    fn select_worker_for_job(&self, job: &ExecutionJob) -> Result<WorkerId, Error> {
        // Get current worker statuses
        let worker_statuses = self.worker_pool.worker_status.read().unwrap();
        let idle_workers: Vec<_> = worker_statuses.iter()
            .enumerate()
            .filter(|(_, status)| matches!(status, WorkerStatus::Idle))
            .map(|(idx, _)| WorkerId(idx))
            .collect();
        
        if idle_workers.is_empty() {
            return Err(Error::NoAvailableWorkers);
        }
        
        // Select based on strategy
        match &self.strategy {
            LoadBalancingStrategy::RoundRobin { next_worker } => {
                // Simple round-robin
                let worker_count = worker_statuses.len();
                let current = next_worker.fetch_add(1, Ordering::Relaxed) % worker_count;
                
                // Find next available worker starting from the current position
                for offset in 0..worker_count {
                    let idx = (current + offset) % worker_count;
                    if let WorkerStatus::Idle = worker_statuses[idx] {
                        return Ok(WorkerId(idx));
                    }
                }
                
                // Should not reach here if idle_workers is non-empty
                Err(Error::NoAvailableWorkers)
            },
            
            LoadBalancingStrategy::LeastLoaded => {
                // Find worker with fewest assigned jobs
                let jobs_per_worker = self.stats.jobs_per_worker.read().unwrap();
                
                let mut min_jobs = u64::MAX;
                let mut min_worker = None;
                
                for worker_id in idle_workers {
                    let jobs = jobs_per_worker[worker_id.0];
                    if jobs < min_jobs {
                        min_jobs = jobs;
                        min_worker = Some(worker_id);
                    }
                }
                
                min_worker.ok_or(Error::NoAvailableWorkers)
            },
            
            LoadBalancingStrategy::FastestWorker => {
                // Find worker with lowest average execution time
                let times = self.stats.execution_time_per_worker.read().unwrap();
                
                let mut fastest_time = Duration::from_secs(u64::MAX);
                let mut fastest_worker = None;
                
                for worker_id in idle_workers {
                    let time = times[worker_id.0];
                    if time < fastest_time {
                        fastest_time = time;
                        fastest_worker = Some(worker_id);
                    }
                }
                
                fastest_worker.ok_or(Error::NoAvailableWorkers)
            },
            
            LoadBalancingStrategy::JobAffinity { affinity_function } => {
                // Check if job has affinity for a specific worker
                if let Some(worker_id) = affinity_function(job) {
                    // Ensure worker is idle
                    if let WorkerStatus::Idle = worker_statuses.get(worker_id.0)
                                                        .ok_or(Error::InvalidWorkerId)? {
                        return Ok(worker_id);
                    }
                }
                
                // Fall back to least loaded strategy
                let jobs_per_worker = self.stats.jobs_per_worker.read().unwrap();
                
                let mut min_jobs = u64::MAX;
                let mut min_worker = None;
                
                for worker_id in idle_workers {
                    let jobs = jobs_per_worker[worker_id.0];
                    if jobs < min_jobs {
                        min_jobs = jobs;
                        min_worker = Some(worker_id);
                    }
                }
                
                min_worker.ok_or(Error::NoAvailableWorkers)
            },
        }
    }
}
```

## Scheduling Algorithm Implementation

```rust
impl ExecutionScheduler {
    /// Schedule a batch of transactions for execution
    fn schedule_batch(&mut self, transactions: Vec<Transaction>) -> Result<(), Error> {
        // Convert transactions to jobs
        let jobs = self.create_jobs(transactions)?;
        
        // Create dependency graph
        let dependency_graph = self.analyze_dependencies(&jobs)?;
        
        // Update job dependencies
        let jobs_with_deps = self.apply_dependencies(jobs, &dependency_graph)?;
        
        // Add jobs to queue
        for job in jobs_with_deps {
            self.add_job(job)?;
        }
        
        // Start scheduling process
        self.run_scheduling_cycle()?;
        
        Ok(())
    }
    
    /// Create execution jobs from transactions
    fn create_jobs(&self, transactions: Vec<Transaction>) -> Result<Vec<ExecutionJob>, Error> {
        let mut jobs = Vec::with_capacity(transactions.len());
        
        for tx in transactions {
            // Calculate priority based on gas price and other factors
            let priority = self.prioritizer.calculate_priority(&tx);
            
            // Estimate execution duration
            let estimated_duration = self.estimate_execution_time(&tx);
            
            // Create job
            let job = ExecutionJob {
                id: JobId::new(),
                transaction: tx,
                creation_time: Instant::now(),
                priority,
                estimated_duration_ns: estimated_duration.as_nanos() as u64,
                dependencies: HashSet::new(),
                status: JobStatus::Created,
            };
            
            jobs.push(job);
        }
        
        Ok(jobs)
    }
    
    /// Analyze dependencies between jobs
    fn analyze_dependencies(&self, jobs: &[ExecutionJob]) -> Result<DependencyGraph<JobId>, Error> {
        let mut graph = DependencyGraph::new();
        
        // First add all jobs as nodes
        for job in jobs {
            graph.add_node(job.id);
        }
        
        // Group by sender for nonce dependencies
        let mut jobs_by_sender: HashMap<Address, Vec<&ExecutionJob>> = HashMap::new();
        
        for job in jobs {
            let sender = job.transaction.sender;
            jobs_by_sender.entry(sender).or_default().push(job);
        }
        
        // Add nonce dependencies between jobs from the same sender
        for (_, sender_jobs) in &jobs_by_sender {
            let mut sorted_jobs = sender_jobs.clone();
            sorted_jobs.sort_by_key(|job| job.transaction.nonce);
            
            for i in 1..sorted_jobs.len() {
                let prev_job = sorted_jobs[i - 1];
                let curr_job = sorted_jobs[i];
                
                // Add dependency edge from current job to previous job
                // (current depends on previous completing first)
                graph.add_edge(prev_job.id, curr_job.id);
            }
        }
        
        Ok(graph)
    }
    
    /// Apply dependency information to jobs
    fn apply_dependencies(
        &self,
        mut jobs: Vec<ExecutionJob>,
        dependency_graph: &DependencyGraph<JobId>
    ) -> Result<Vec<ExecutionJob>, Error> {
        // Create mapping from job ID to index in the jobs vector
        let job_indices: HashMap<_, _> = jobs.iter()
            .enumerate()
            .map(|(idx, job)| (job.id, idx))
            .collect();
        
        // Add dependency information to each job
        for job_idx in 0..jobs.len() {
            let job_id = jobs[job_idx].id;
            
            // Get dependencies for this job
            let dependencies = dependency_graph.get_dependencies(job_id);
            
            // Add to job's dependency set
            jobs[job_idx].dependencies = dependencies.cloned().collect();
        }
        
        Ok(jobs)
    }
    
    /// Add a job to the execution queue
    fn add_job(&self, mut job: ExecutionJob) -> Result<(), Error> {
        // Update job status
        if job.dependencies.is_empty() {
            job.status = JobStatus::Ready;
        }
        
        // Add to job queue
        self.worker_pool.job_queue.push(job)
    }
    
    /// Main scheduling cycle
    fn run_scheduling_cycle(&self) -> Result<(), Error> {
        // Check for ready jobs
        let ready_jobs = self.get_ready_jobs()?;
        
        // Assign jobs to workers
        for job in ready_jobs {
            let selected_worker = self.load_balancer.select_worker_for_job(&job)?;
            self.assign_job_to_worker(job, selected_worker)?;
        }
        
        Ok(())
    }
    
    /// Get jobs that are ready for execution
    fn get_ready_jobs(&self) -> Result<Vec<ExecutionJob>, Error> {
        let mut ready_jobs = Vec::new();
        
        // Get jobs from the queue
        while let Some(job) = self.worker_pool.job_queue.pop_ready() {
            let mut job_guard = job.lock().unwrap();
            
            // Check if job is already running
            if let JobStatus::Ready = job_guard.status {
                ready_jobs.push(job_guard.clone());
                
                // Stop if we have enough jobs for this cycle
                if ready_jobs.len() >= self.worker_pool.config.max_jobs_per_cycle {
                    break;
                }
            }
        }
        
        Ok(ready_jobs)
    }
    
    /// Assign job to a specific worker
    fn assign_job_to_worker(&self, job: ExecutionJob, worker_id: WorkerId) -> Result<(), Error> {
        // Wrap job in thread-safe container
        let job_arc = Arc::new(Mutex::new(job));
        
        // Send job to worker
        let worker = &self.worker_pool.workers[worker_id.0];
        worker.sender.send(WorkerCommand::ExecuteJob(job_arc)).map_err(|_| Error::WorkerCommunicationError)?;
        
        // Update stats
        {
            let mut jobs_per_worker = self.load_balancer.stats.jobs_per_worker.write().unwrap();
            jobs_per_worker[worker_id.0] += 1;
        }
        
        // Update metrics
        self.metrics.scheduled_transactions.fetch_add(1, Ordering::Relaxed);
        
        Ok(())
    }
    
    /// Estimate execution time for a transaction
    fn estimate_execution_time(&self, tx: &Transaction) -> Duration {
        // In a real implementation, this would use historical data and transaction properties
        // For this example, we'll use a simple heuristic based on gas limit
        
        // Base time for any transaction
        let base_ns = 50_000;
        
        // Time proportional to gas limit
        let gas_ns = tx.gas_limit * 10;
        
        // Extra time for contract calls
        let contract_ns = if tx.transaction_type == TransactionType::ContractCall { 200_000 } else { 0 };
        
        Duration::from_nanos(base_ns + gas_ns + contract_ns)
    }
}
```

**Design Rationale**:
- **Flexible Scheduling**: Multiple policies to handle different workload patterns
- **Load Balancing**: Distributes work to maximize resource utilization
- **Priority Management**: Ensures high-priority transactions get processed first
- **Dependency Awareness**: Schedules transactions respecting their dependencies

**For Beginners**: Execution scheduling is like an air traffic controller for transactions. It decides which transactions get processed when and by which "worker" (computing resource). The scheduler makes sure high-fee transactions get priority, transactions from the same sender are processed in the correct order, and computing resources are used efficiently. Without good scheduling, the system would either leave processors idle or create bottlenecks with too many transactions assigned to the same worker.

[Back to Chapter 8](./06.08-transaction-layer-parallelization.md) | [Previous: Concurrency Models](./06.08.2-transaction-layer-concurrency-models.md) | [Next: Security](./06.09-transaction-layer-security.md)
